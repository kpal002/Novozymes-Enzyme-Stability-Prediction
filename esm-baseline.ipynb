{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kuntalpal/few-baselines-without-much-effort?scriptVersionId=115097228\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"%%capture\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T12:05:07.703961Z","iopub.execute_input":"2022-12-30T12:05:07.705095Z","iopub.status.idle":"2022-12-30T12:05:19.03743Z","shell.execute_reply.started":"2022-12-30T12:05:07.705035Z","shell.execute_reply":"2022-12-30T12:05:19.035961Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Physics behind Protein Stability","metadata":{}},{"cell_type":"markdown","source":"**DDG** : Delta Delta G (DDG) is a metric for predicting how a single point mutation will affect protein stability. The thermodynamic stability change upon mutation is measured by ΔΔG(T$_{r}$), i.e. the difference between the standard Gibbs folding free energies of the mutant (ΔG$^{mut}$) and wild-type (ΔG$^{wild}$) proteins at the reference temperature T$_{r}$:\n\n$$  ΔΔG(T_{r}) = (ΔG^{mut})(T_{r}) - (ΔG^{wild})(T_{r}) $$\n\nIn general,\n\n$$ G = Enthalpy \\ \\  (H) - Temperature \\ \\ (T) \\ x \\ Entropy \\ \\ (S)  $$\n\nH is the internal energy of a protein. H decreases during protein folding because folding will cause packing of hydrophobics, optimized polar group orientation, and achieves a good proximity to ideal bond lengths and angles. S is the measure of order within a system. S of a protein becomes lower during protein folding because residue dynamics will be significantly decreased in comparison the unfolded state. However, decrease is in protein S will also cause an increase in S for solvent. The energy of an unfolded protein is nearly identical with a single point mutation. So, the\ndominant factor in DDG is the energy difference of the folded state.\n","metadata":{}},{"cell_type":"markdown","source":"DDG results will fall into three categories:\n\n1. **DDG > 0.5**: Positive results suggest that a mutation would be destabilizing. Most mutations will be positive or close to zero because proteins have evolved to be reasonably stable. These mutations are residues that you should usually avoid during design.\n\n2. Things that are near 0 are within the noise range so should be considered neutral.\n\n3. DDG < -0.5: Negative results suggest that the mutation would lead to a more stable protein. However, the environment at each position should be considered.\n\n    a. If interacting molecules are not present in the model, such as at a known zinc binding site, then a seemingly favorable mutation will not be favorable in reality.\n    \n    b. A positions that has a lot of negative DDGs could mean that this position evolved a destabilizing residue because it is necessary for its catalytic activity, for binding another molecule, or because of another functionally relevant reason.\n    \n    c. Also, consider that this measures a single point mutation. Many times it requires multiple interacting mutations in order to achieve significant stability.","metadata":{}},{"cell_type":"markdown","source":"# ESM-2\n\nESM-2 is a transformer-based language model, and uses an attention mechanism to learn interaction patterns between pairs of amino acids in the input sequence.\n\nThe larger the value of the attention, the more impactful would be any mutation at that location. So the melting temperature is inversely proportional to the sum of the contact attentions.","metadata":{}},{"cell_type":"code","source":"# # latest release\n# !pip install fair-esm","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:19.040079Z","iopub.execute_input":"2022-12-30T12:05:19.04044Z","iopub.status.idle":"2022-12-30T12:05:19.050882Z","shell.execute_reply.started":"2022-12-30T12:05:19.040408Z","shell.execute_reply":"2022-12-30T12:05:19.047347Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# #set the edit_idx\n# import pandas as pd, numpy as np, nltk\n# from sklearn.model_selection import GroupKFold\n# import torch\n# import esm\n#wt = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n#df_test = pd.read_csv(\"../input/novozymes-enzyme-stability-prediction/test.csv\")\n#df_test.loc[1169, 'protein_sequence'] = wt[:-1]+\"!\" #1169 is the same as wt\n\n#df_test['edit_idx'] = df_test.apply(lambda x:[i for i in range(len(x['protein_sequence'])) if x['protein_sequence'][i] != wt[i]][0], axis = 1)\n\n\n# # Load ESM-2 model\n# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n# batch_converter = alphabet.get_batch_converter()\n# model.eval()  # disables dropout for deterministic results\n\n\n# # Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n# data = [\n#     (\"protein1\", wt),\n# ]\n# batch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# # Extract per-residue representations (on CPU)\n# with torch.no_grad():\n#     results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n# token_representations = results[\"representations\"][33]\n\n# # Generate per-sequence representations via averaging\n# # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n# sequence_representations = []\n# for i, (_, seq) in enumerate(data):\n#     sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n\n    \n# #all the code above is from ESM QuickStart\n# ac_sum = np.sum(np.array(results[\"contacts\"][0]), axis=1) \n# test_df['tm'] = test_df.apply(lambda x:1/ac_sum[x['edit_idx']], axis = 1)  \n# test_df[['seq_id', 'tm']].set_index(\"seq_id\").to_csv(\"submission.csv\")\n#df_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:19.054737Z","iopub.execute_input":"2022-12-30T12:05:19.055827Z","iopub.status.idle":"2022-12-30T12:05:19.065027Z","shell.execute_reply.started":"2022-12-30T12:05:19.055767Z","shell.execute_reply":"2022-12-30T12:05:19.062809Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/nesp-public-train-sets/Q3214/Q3214_direct.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:19.067388Z","iopub.execute_input":"2022-12-30T12:05:19.068122Z","iopub.status.idle":"2022-12-30T12:05:19.107183Z","shell.execute_reply.started":"2022-12-30T12:05:19.068075Z","shell.execute_reply":"2022-12-30T12:05:19.105835Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"     pdb_id  position wild_type mutant   ddg\n0     1otrB        34         E      A -0.07\n1     1a5eA       121         L      R -0.66\n2     1rtbA         4         A      S  0.47\n3     4lyzA       102         G      R -0.38\n4     1thqA       157         M      A  0.77\n...     ...       ...       ...    ...   ...\n3209  2lzmA        42         A      K  3.70\n3210  1yeaA        76         P      G  1.20\n3211  1stnA       104         V      T  2.50\n3212  2lzmA        71         V      A  1.50\n3213  1vqbA        86         A      T  0.70\n\n[3214 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pdb_id</th>\n      <th>position</th>\n      <th>wild_type</th>\n      <th>mutant</th>\n      <th>ddg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1otrB</td>\n      <td>34</td>\n      <td>E</td>\n      <td>A</td>\n      <td>-0.07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1a5eA</td>\n      <td>121</td>\n      <td>L</td>\n      <td>R</td>\n      <td>-0.66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1rtbA</td>\n      <td>4</td>\n      <td>A</td>\n      <td>S</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4lyzA</td>\n      <td>102</td>\n      <td>G</td>\n      <td>R</td>\n      <td>-0.38</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1thqA</td>\n      <td>157</td>\n      <td>M</td>\n      <td>A</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3209</th>\n      <td>2lzmA</td>\n      <td>42</td>\n      <td>A</td>\n      <td>K</td>\n      <td>3.70</td>\n    </tr>\n    <tr>\n      <th>3210</th>\n      <td>1yeaA</td>\n      <td>76</td>\n      <td>P</td>\n      <td>G</td>\n      <td>1.20</td>\n    </tr>\n    <tr>\n      <th>3211</th>\n      <td>1stnA</td>\n      <td>104</td>\n      <td>V</td>\n      <td>T</td>\n      <td>2.50</td>\n    </tr>\n    <tr>\n      <th>3212</th>\n      <td>2lzmA</td>\n      <td>71</td>\n      <td>V</td>\n      <td>A</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>3213</th>\n      <td>1vqbA</td>\n      <td>86</td>\n      <td>A</td>\n      <td>T</td>\n      <td>0.70</td>\n    </tr>\n  </tbody>\n</table>\n<p>3214 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%capture\n!pip install Levenshtein\nimport math\nimport multiprocessing\nimport os\nimport sys\n\nimport Levenshtein\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport tensorflow as tf\nfrom keras import layers, callbacks\nfrom keras import models\nfrom keras import optimizers\nfrom sklearn.model_selection import GroupKFold\nfrom keras.saving.save import load_model\nfrom tqdm import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef gen_mutations(name, df,\n                  wild=\"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQ\"\"RVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGT\"\"NAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKAL\"\"GSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"):\n    result = []\n    for _, r in df.iterrows():\n        ops = Levenshtein.editops(wild, r.protein_sequence)\n        assert len(ops) <= 1\n        if len(ops) > 0 and ops[0][0] == 'replace':\n            idx = ops[0][1]\n            result.append([ops[0][0], idx + 1, wild[idx], r.protein_sequence[idx]])\n        elif len(ops) == 0:\n            result.append(['same', 0, '', ''])\n        elif ops[0][0] == 'insert':\n            assert False, \"Ups\"\n        elif ops[0][0] == 'delete':\n            idx = ops[0][1]\n            result.append(['delete', idx + 1, wild[idx], '-'])\n        else:\n            assert False, \"Ups\"\n\n    df = pd.concat([df, pd.DataFrame(data=result, columns=['op', 'idx', 'wild', 'mutant'])], axis=1)\n    df['mut'] = df[['wild', 'idx', 'mutant']].astype(str).apply(lambda v: ''.join(v), axis=1)\n    df['name'] = name\n    return df\n\n\ndf_test = gen_mutations('wildtypeA', pd.read_csv('../input/novozymes-enzyme-stability-prediction/test.csv'))\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:19.114748Z","iopub.execute_input":"2022-12-30T12:05:19.115681Z","iopub.status.idle":"2022-12-30T12:05:19.437952Z","shell.execute_reply.started":"2022-12-30T12:05:19.11564Z","shell.execute_reply":"2022-12-30T12:05:19.437076Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"      seq_id                                   protein_sequence  pH  \\\n0      31390  VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n1      31391  VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2      31392  VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8   \n3      31393  VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n4      31394  VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n...      ...                                                ...  ..   \n2408   33798  VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2409   33799  VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2410   33800  VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2411   33801  VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2412   33802  VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n\n     data_source       op  idx wild mutant   mut       name  \n0      Novozymes  replace   17    L      E  L17E  wildtypeA  \n1      Novozymes  replace   17    L      K  L17K  wildtypeA  \n2      Novozymes   delete   17    L      -  L17-  wildtypeA  \n3      Novozymes  replace   18    K      C  K18C  wildtypeA  \n4      Novozymes  replace   18    K      F  K18F  wildtypeA  \n...          ...      ...  ...  ...    ...   ...        ...  \n2408   Novozymes  replace   16    A      I  A16I  wildtypeA  \n2409   Novozymes  replace   16    A      L  A16L  wildtypeA  \n2410   Novozymes  replace   16    A      N  A16N  wildtypeA  \n2411   Novozymes  replace   16    A      P  A16P  wildtypeA  \n2412   Novozymes  replace   16    A      W  A16W  wildtypeA  \n\n[2413 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seq_id</th>\n      <th>protein_sequence</th>\n      <th>pH</th>\n      <th>data_source</th>\n      <th>op</th>\n      <th>idx</th>\n      <th>wild</th>\n      <th>mutant</th>\n      <th>mut</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>31390</td>\n      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>17</td>\n      <td>L</td>\n      <td>E</td>\n      <td>L17E</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31391</td>\n      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>17</td>\n      <td>L</td>\n      <td>K</td>\n      <td>L17K</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31392</td>\n      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>delete</td>\n      <td>17</td>\n      <td>L</td>\n      <td>-</td>\n      <td>L17-</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31393</td>\n      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>18</td>\n      <td>K</td>\n      <td>C</td>\n      <td>K18C</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31394</td>\n      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>18</td>\n      <td>K</td>\n      <td>F</td>\n      <td>K18F</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>33798</td>\n      <td>VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>I</td>\n      <td>A16I</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>33799</td>\n      <td>VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>L</td>\n      <td>A16L</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>33800</td>\n      <td>VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>N</td>\n      <td>A16N</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>33801</td>\n      <td>VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>P</td>\n      <td>A16P</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>33802</td>\n      <td>VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>W</td>\n      <td>A16W</td>\n      <td>wildtypeA</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"TRAIN_FEATURES_PATH = '../input/thermonet-features/Q3214.npy'\nX = np.load(TRAIN_FEATURES_PATH)\nX = np.moveaxis(X, 1, -1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:19.441886Z","iopub.execute_input":"2022-12-30T12:05:19.444359Z","iopub.status.idle":"2022-12-30T12:05:22.542473Z","shell.execute_reply.started":"2022-12-30T12:05:19.444321Z","shell.execute_reply":"2022-12-30T12:05:22.541433Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(3214, 16, 16, 16, 14)"},"metadata":{}}]},{"cell_type":"code","source":"pdb_ids = df_train.pdb_id\ny = df_train.ddg","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:22.544054Z","iopub.execute_input":"2022-12-30T12:05:22.544466Z","iopub.status.idle":"2022-12-30T12:05:22.549987Z","shell.execute_reply.started":"2022-12-30T12:05:22.544428Z","shell.execute_reply":"2022-12-30T12:05:22.548893Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def gen_model(params):\n    def build_model(params):\n        conv_layer_sizes = params['conv_layer_sizes']\n        dense_layer_size = params['dense_layer_size']\n        dropout_rate = params['dropout_rate']\n        model = models.Sequential()\n        model.add(layers.Conv3D(filters=conv_layer_sizes[0], kernel_size=(3, 3, 3), input_shape=(16, 16, 16, 14)))\n        model.add(layers.Activation(activation='relu'))\n\n        for ls in conv_layer_sizes[1:]:\n            model.add(layers.Conv3D(filters=ls, kernel_size=(3, 3, 3)))\n            model.add(layers.Activation(activation='relu'))\n\n        model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n        model.add(layers.Flatten())\n\n        model.add(layers.Dropout(rate=dropout_rate))\n        model.add(layers.Dense(units=dense_layer_size, activation='relu'))\n        model.add(layers.Dropout(rate=dropout_rate))\n        model.add(layers.Dense(units=1))\n        return model\n\n    model = build_model(params)\n    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(\n        learning_rate=params['learning_rate'],\n        beta_1=0.9,\n        beta_2=0.999,\n        amsgrad=False\n    ), metrics=['mae'])\n    return model\n\n\ndef train_model(X_train, y_train, X_val, y_val, params, path):\n    model = gen_model(params)\n    scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=params['scheduler_factor'],\n        patience=params['scheduler_patience'],\n        verbose=0,\n        mode=\"auto\",\n        min_delta=0.0001,\n        cooldown=0,\n        min_lr=1e-5,\n    )\n\n    checkpoint = callbacks.ModelCheckpoint(path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=params['early_stopping_patience'])\n    result = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                       epochs=EPOCHS, batch_size=params['batch_size'], verbose=1, callbacks=[scheduler, checkpoint, early_stopping])\n    return load_model(path), result\n\nN_FOLDS = 5\nGROUP_KFOLD = False\nEPOCHS = 200\nMODELS_PATH = 'models'\n\n\n\n\nPARAMS = {\n    'conv_layer_sizes': (16, 24, 32),\n    'dense_layer_size': 24,\n    'dropout_rate': 0.5,\n    'learning_rate': 0.001,\n    'batch_size': 8,\n    'scheduler_patience': 10,\n    'scheduler_factor': math.sqrt(0.1),\n    'early_stopping_patience': 20,\n}\n\n\n!mkdir -p models\nkfold = GroupKFold(N_FOLDS)\nthermonet_models = []\nval_losses = []\nif GROUP_KFOLD:\n    groups = pdb_ids\nelse:\n    groups = range(len(pdb_ids))\n\nfor fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(X, y, groups=groups), total=N_FOLDS, desc=\"Folds\")):\n    X_train = X[train_idx]\n    y_train = y[train_idx]\n    X_val = X[val_idx]\n    y_val = y[val_idx]\n    path = f'{MODELS_PATH}/model{fold}.h5'\n    model, result = train_model(X_train, y_train, X_val, y_val, PARAMS, path)\n    thermonet_models.append(model)\n    val_losses.append(result.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:05:22.55189Z","iopub.execute_input":"2022-12-30T12:05:22.552688Z","iopub.status.idle":"2022-12-30T12:17:34.122433Z","shell.execute_reply.started":"2022-12-30T12:05:22.552651Z","shell.execute_reply":"2022-12-30T12:17:34.120536Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Folds:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n322/322 [==============================] - 3s 8ms/step - loss: 4.1881 - mae: 1.4129 - val_loss: 4.1807 - val_mae: 1.3915\n\nEpoch 00001: val_loss improved from inf to 4.18068, saving model to models/model0.h5\nEpoch 2/200\n322/322 [==============================] - 2s 7ms/step - loss: 3.8951 - mae: 1.3395 - val_loss: 4.1644 - val_mae: 1.3701\n\nEpoch 00002: val_loss improved from 4.18068 to 4.16438, saving model to models/model0.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.6028 - mae: 1.2860 - val_loss: 3.7008 - val_mae: 1.2844\n\nEpoch 00003: val_loss improved from 4.16438 to 3.70075, saving model to models/model0.h5\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3816 - mae: 1.2525 - val_loss: 3.6979 - val_mae: 1.2919\n\nEpoch 00004: val_loss improved from 3.70075 to 3.69789, saving model to models/model0.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1583 - mae: 1.2216 - val_loss: 3.8329 - val_mae: 1.3080\n\nEpoch 00005: val_loss did not improve from 3.69789\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.0853 - mae: 1.1909 - val_loss: 3.6656 - val_mae: 1.2734\n\nEpoch 00006: val_loss improved from 3.69789 to 3.66561, saving model to models/model0.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8827 - mae: 1.1662 - val_loss: 3.5894 - val_mae: 1.2387\n\nEpoch 00007: val_loss improved from 3.66561 to 3.58943, saving model to models/model0.h5\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8029 - mae: 1.1443 - val_loss: 3.4900 - val_mae: 1.2163\n\nEpoch 00008: val_loss improved from 3.58943 to 3.49002, saving model to models/model0.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5979 - mae: 1.1177 - val_loss: 3.4546 - val_mae: 1.2131\n\nEpoch 00009: val_loss improved from 3.49002 to 3.45460, saving model to models/model0.h5\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4245 - mae: 1.0947 - val_loss: 3.4168 - val_mae: 1.2039\n\nEpoch 00010: val_loss improved from 3.45460 to 3.41681, saving model to models/model0.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4257 - mae: 1.1016 - val_loss: 3.6108 - val_mae: 1.2569\n\nEpoch 00011: val_loss did not improve from 3.41681\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3141 - mae: 1.0666 - val_loss: 3.4438 - val_mae: 1.2028\n\nEpoch 00012: val_loss did not improve from 3.41681\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1327 - mae: 1.0425 - val_loss: 3.3957 - val_mae: 1.2111\n\nEpoch 00013: val_loss improved from 3.41681 to 3.39572, saving model to models/model0.h5\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1599 - mae: 1.0311 - val_loss: 3.4209 - val_mae: 1.2077\n\nEpoch 00014: val_loss did not improve from 3.39572\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9864 - mae: 0.9996 - val_loss: 3.4191 - val_mae: 1.2035\n\nEpoch 00015: val_loss did not improve from 3.39572\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8974 - mae: 0.9754 - val_loss: 3.4311 - val_mae: 1.2118\n\nEpoch 00016: val_loss did not improve from 3.39572\nEpoch 17/200\n322/322 [==============================] - 2s 7ms/step - loss: 2.0813 - mae: 1.0100 - val_loss: 3.4310 - val_mae: 1.2115\n\nEpoch 00017: val_loss did not improve from 3.39572\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9651 - mae: 0.9771 - val_loss: 3.4112 - val_mae: 1.2028\n\nEpoch 00018: val_loss did not improve from 3.39572\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9378 - mae: 0.9762 - val_loss: 3.3802 - val_mae: 1.2070\n\nEpoch 00019: val_loss improved from 3.39572 to 3.38019, saving model to models/model0.h5\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6519 - mae: 0.9324 - val_loss: 3.4015 - val_mae: 1.2264\n\nEpoch 00020: val_loss did not improve from 3.38019\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7378 - mae: 0.9459 - val_loss: 3.4949 - val_mae: 1.2223\n\nEpoch 00021: val_loss did not improve from 3.38019\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6659 - mae: 0.9174 - val_loss: 3.3831 - val_mae: 1.1996\n\nEpoch 00022: val_loss did not improve from 3.38019\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5941 - mae: 0.9113 - val_loss: 3.3695 - val_mae: 1.1901\n\nEpoch 00023: val_loss improved from 3.38019 to 3.36948, saving model to models/model0.h5\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6572 - mae: 0.9213 - val_loss: 3.2883 - val_mae: 1.1962\n\nEpoch 00024: val_loss improved from 3.36948 to 3.28835, saving model to models/model0.h5\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5929 - mae: 0.9029 - val_loss: 3.3889 - val_mae: 1.2079\n\nEpoch 00025: val_loss did not improve from 3.28835\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6112 - mae: 0.9171 - val_loss: 3.3347 - val_mae: 1.1842\n\nEpoch 00026: val_loss did not improve from 3.28835\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3886 - mae: 0.8722 - val_loss: 3.3936 - val_mae: 1.1949\n\nEpoch 00027: val_loss did not improve from 3.28835\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5749 - mae: 0.8956 - val_loss: 3.3601 - val_mae: 1.2135\n\nEpoch 00028: val_loss did not improve from 3.28835\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3837 - mae: 0.8642 - val_loss: 3.5486 - val_mae: 1.2406\n\nEpoch 00029: val_loss did not improve from 3.28835\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4470 - mae: 0.8791 - val_loss: 3.5873 - val_mae: 1.2510\n\nEpoch 00030: val_loss did not improve from 3.28835\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4718 - mae: 0.8884 - val_loss: 3.3425 - val_mae: 1.2033\n\nEpoch 00031: val_loss did not improve from 3.28835\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5109 - mae: 0.8715 - val_loss: 3.3822 - val_mae: 1.2185\n\nEpoch 00032: val_loss did not improve from 3.28835\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3248 - mae: 0.8269 - val_loss: 3.2550 - val_mae: 1.1785\n\nEpoch 00033: val_loss improved from 3.28835 to 3.25500, saving model to models/model0.h5\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5379 - mae: 0.8841 - val_loss: 3.3002 - val_mae: 1.1878\n\nEpoch 00034: val_loss did not improve from 3.25500\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2417 - mae: 0.8127 - val_loss: 3.4804 - val_mae: 1.2156\n\nEpoch 00035: val_loss did not improve from 3.25500\nEpoch 36/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.2341 - mae: 0.8232 - val_loss: 3.3492 - val_mae: 1.1819\n\nEpoch 00036: val_loss did not improve from 3.25500\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1970 - mae: 0.7894 - val_loss: 3.3011 - val_mae: 1.1836\n\nEpoch 00037: val_loss did not improve from 3.25500\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3208 - mae: 0.8061 - val_loss: 3.3283 - val_mae: 1.1897\n\nEpoch 00038: val_loss did not improve from 3.25500\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3627 - mae: 0.8272 - val_loss: 3.4172 - val_mae: 1.2113\n\nEpoch 00039: val_loss did not improve from 3.25500\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3609 - mae: 0.8455 - val_loss: 3.3153 - val_mae: 1.1773\n\nEpoch 00040: val_loss did not improve from 3.25500\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4144 - mae: 0.8420 - val_loss: 3.2991 - val_mae: 1.1930\n\nEpoch 00041: val_loss did not improve from 3.25500\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2470 - mae: 0.8126 - val_loss: 3.2827 - val_mae: 1.1937\n\nEpoch 00042: val_loss did not improve from 3.25500\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2255 - mae: 0.8067 - val_loss: 3.3065 - val_mae: 1.1870\n\nEpoch 00043: val_loss did not improve from 3.25500\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2425 - mae: 0.7688 - val_loss: 3.2732 - val_mae: 1.1852\n\nEpoch 00044: val_loss did not improve from 3.25500\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9717 - mae: 0.7270 - val_loss: 3.2661 - val_mae: 1.1798\n\nEpoch 00045: val_loss did not improve from 3.25500\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0068 - mae: 0.7201 - val_loss: 3.2652 - val_mae: 1.1901\n\nEpoch 00046: val_loss did not improve from 3.25500\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9418 - mae: 0.7117 - val_loss: 3.2834 - val_mae: 1.1828\n\nEpoch 00047: val_loss did not improve from 3.25500\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9315 - mae: 0.6945 - val_loss: 3.2469 - val_mae: 1.1798\n\nEpoch 00048: val_loss improved from 3.25500 to 3.24690, saving model to models/model0.h5\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0028 - mae: 0.7161 - val_loss: 3.2127 - val_mae: 1.1707\n\nEpoch 00049: val_loss improved from 3.24690 to 3.21266, saving model to models/model0.h5\nEpoch 50/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9746 - mae: 0.7098 - val_loss: 3.2013 - val_mae: 1.1618\n\nEpoch 00050: val_loss improved from 3.21266 to 3.20132, saving model to models/model0.h5\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9716 - mae: 0.6933 - val_loss: 3.2080 - val_mae: 1.1614\n\nEpoch 00051: val_loss did not improve from 3.20132\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8967 - mae: 0.6844 - val_loss: 3.2101 - val_mae: 1.1657\n\nEpoch 00052: val_loss did not improve from 3.20132\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9055 - mae: 0.6874 - val_loss: 3.2190 - val_mae: 1.1602\n\nEpoch 00053: val_loss did not improve from 3.20132\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9652 - mae: 0.6836 - val_loss: 3.2188 - val_mae: 1.1654\n\nEpoch 00054: val_loss did not improve from 3.20132\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8653 - mae: 0.6604 - val_loss: 3.2392 - val_mae: 1.1678\n\nEpoch 00055: val_loss did not improve from 3.20132\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9173 - mae: 0.6828 - val_loss: 3.2590 - val_mae: 1.1731\n\nEpoch 00056: val_loss did not improve from 3.20132\nEpoch 57/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8929 - mae: 0.6771 - val_loss: 3.2303 - val_mae: 1.1617\n\nEpoch 00057: val_loss did not improve from 3.20132\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9388 - mae: 0.6668 - val_loss: 3.2172 - val_mae: 1.1602\n\nEpoch 00058: val_loss did not improve from 3.20132\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8854 - mae: 0.6696 - val_loss: 3.2650 - val_mae: 1.1754\n\nEpoch 00059: val_loss did not improve from 3.20132\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8633 - mae: 0.6816 - val_loss: 3.2248 - val_mae: 1.1680\n\nEpoch 00060: val_loss did not improve from 3.20132\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9455 - mae: 0.6779 - val_loss: 3.2157 - val_mae: 1.1672\n\nEpoch 00061: val_loss did not improve from 3.20132\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8452 - mae: 0.6539 - val_loss: 3.2122 - val_mae: 1.1628\n\nEpoch 00062: val_loss did not improve from 3.20132\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7701 - mae: 0.6358 - val_loss: 3.2099 - val_mae: 1.1615\n\nEpoch 00063: val_loss did not improve from 3.20132\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8181 - mae: 0.6448 - val_loss: 3.1961 - val_mae: 1.1631\n\nEpoch 00064: val_loss improved from 3.20132 to 3.19614, saving model to models/model0.h5\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9125 - mae: 0.6646 - val_loss: 3.2013 - val_mae: 1.1659\n\nEpoch 00065: val_loss did not improve from 3.19614\nEpoch 66/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.7907 - mae: 0.6367 - val_loss: 3.1921 - val_mae: 1.1613\n\nEpoch 00066: val_loss improved from 3.19614 to 3.19205, saving model to models/model0.h5\nEpoch 67/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8128 - mae: 0.6325 - val_loss: 3.2052 - val_mae: 1.1607\n\nEpoch 00067: val_loss did not improve from 3.19205\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8579 - mae: 0.6510 - val_loss: 3.2155 - val_mae: 1.1612\n\nEpoch 00068: val_loss did not improve from 3.19205\nEpoch 69/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8241 - mae: 0.6350 - val_loss: 3.2043 - val_mae: 1.1592\n\nEpoch 00069: val_loss did not improve from 3.19205\nEpoch 70/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.8935 - mae: 0.6420 - val_loss: 3.2110 - val_mae: 1.1645\n\nEpoch 00070: val_loss did not improve from 3.19205\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8236 - mae: 0.6370 - val_loss: 3.1984 - val_mae: 1.1574\n\nEpoch 00071: val_loss did not improve from 3.19205\nEpoch 72/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7908 - mae: 0.6317 - val_loss: 3.2007 - val_mae: 1.1655\n\nEpoch 00072: val_loss did not improve from 3.19205\nEpoch 73/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7479 - mae: 0.6268 - val_loss: 3.1900 - val_mae: 1.1599\n\nEpoch 00073: val_loss improved from 3.19205 to 3.18996, saving model to models/model0.h5\nEpoch 74/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8608 - mae: 0.6417 - val_loss: 3.1949 - val_mae: 1.1572\n\nEpoch 00074: val_loss did not improve from 3.18996\nEpoch 75/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7547 - mae: 0.6041 - val_loss: 3.1810 - val_mae: 1.1550\n\nEpoch 00075: val_loss improved from 3.18996 to 3.18098, saving model to models/model0.h5\nEpoch 76/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7895 - mae: 0.6286 - val_loss: 3.1888 - val_mae: 1.1556\n\nEpoch 00076: val_loss did not improve from 3.18098\nEpoch 77/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6897 - mae: 0.5993 - val_loss: 3.1861 - val_mae: 1.1531\n\nEpoch 00077: val_loss did not improve from 3.18098\nEpoch 78/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7354 - mae: 0.6190 - val_loss: 3.1885 - val_mae: 1.1594\n\nEpoch 00078: val_loss did not improve from 3.18098\nEpoch 79/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7552 - mae: 0.6164 - val_loss: 3.2030 - val_mae: 1.1609\n\nEpoch 00079: val_loss did not improve from 3.18098\nEpoch 80/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8322 - mae: 0.6234 - val_loss: 3.1843 - val_mae: 1.1513\n\nEpoch 00080: val_loss did not improve from 3.18098\nEpoch 81/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8213 - mae: 0.6232 - val_loss: 3.1860 - val_mae: 1.1506\n\nEpoch 00081: val_loss did not improve from 3.18098\nEpoch 82/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.7992 - mae: 0.6150 - val_loss: 3.2001 - val_mae: 1.1593\n\nEpoch 00082: val_loss did not improve from 3.18098\nEpoch 83/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7657 - mae: 0.6141 - val_loss: 3.2043 - val_mae: 1.1587\n\nEpoch 00083: val_loss did not improve from 3.18098\nEpoch 84/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8400 - mae: 0.6294 - val_loss: 3.2105 - val_mae: 1.1612\n\nEpoch 00084: val_loss did not improve from 3.18098\nEpoch 85/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7609 - mae: 0.6185 - val_loss: 3.2068 - val_mae: 1.1612\n\nEpoch 00085: val_loss did not improve from 3.18098\nEpoch 86/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7536 - mae: 0.6144 - val_loss: 3.2027 - val_mae: 1.1611\n\nEpoch 00086: val_loss did not improve from 3.18098\nEpoch 87/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7141 - mae: 0.6154 - val_loss: 3.2026 - val_mae: 1.1604\n\nEpoch 00087: val_loss did not improve from 3.18098\nEpoch 88/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7723 - mae: 0.6083 - val_loss: 3.2015 - val_mae: 1.1600\n\nEpoch 00088: val_loss did not improve from 3.18098\nEpoch 89/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9064 - mae: 0.6144 - val_loss: 3.2156 - val_mae: 1.1630\n\nEpoch 00089: val_loss did not improve from 3.18098\nEpoch 90/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8291 - mae: 0.6103 - val_loss: 3.2213 - val_mae: 1.1641\n\nEpoch 00090: val_loss did not improve from 3.18098\nEpoch 91/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7958 - mae: 0.6355 - val_loss: 3.2144 - val_mae: 1.1617\n\nEpoch 00091: val_loss did not improve from 3.18098\nEpoch 92/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7867 - mae: 0.6131 - val_loss: 3.2174 - val_mae: 1.1629\n\nEpoch 00092: val_loss did not improve from 3.18098\nEpoch 93/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6898 - mae: 0.5913 - val_loss: 3.2128 - val_mae: 1.1611\n\nEpoch 00093: val_loss did not improve from 3.18098\nEpoch 94/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7132 - mae: 0.5891 - val_loss: 3.2171 - val_mae: 1.1619\n\nEpoch 00094: val_loss did not improve from 3.18098\nEpoch 95/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7628 - mae: 0.6208 - val_loss: 3.2147 - val_mae: 1.1607\n","output_type":"stream"},{"name":"stderr","text":"Folds:  20%|██        | 1/5 [03:07<12:30, 187.53s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00095: val_loss did not improve from 3.18098\nEpoch 1/200\n322/322 [==============================] - 3s 7ms/step - loss: 4.4311 - mae: 1.4498 - val_loss: 3.6809 - val_mae: 1.3297\n\nEpoch 00001: val_loss improved from inf to 3.68090, saving model to models/model1.h5\nEpoch 2/200\n322/322 [==============================] - 2s 8ms/step - loss: 4.1021 - mae: 1.3866 - val_loss: 3.3864 - val_mae: 1.2797\n\nEpoch 00002: val_loss improved from 3.68090 to 3.38637, saving model to models/model1.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.9236 - mae: 1.3369 - val_loss: 4.1127 - val_mae: 1.3809\n\nEpoch 00003: val_loss did not improve from 3.38637\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7343 - mae: 1.3095 - val_loss: 2.9984 - val_mae: 1.1988\n\nEpoch 00004: val_loss improved from 3.38637 to 2.99837, saving model to models/model1.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.5177 - mae: 1.2692 - val_loss: 3.1084 - val_mae: 1.2121\n\nEpoch 00005: val_loss did not improve from 2.99837\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4082 - mae: 1.2426 - val_loss: 3.0496 - val_mae: 1.2019\n\nEpoch 00006: val_loss did not improve from 2.99837\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2944 - mae: 1.2238 - val_loss: 2.8790 - val_mae: 1.1868\n\nEpoch 00007: val_loss improved from 2.99837 to 2.87903, saving model to models/model1.h5\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2476 - mae: 1.2142 - val_loss: 3.0441 - val_mae: 1.2142\n\nEpoch 00008: val_loss did not improve from 2.87903\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1790 - mae: 1.2032 - val_loss: 2.9830 - val_mae: 1.2219\n\nEpoch 00009: val_loss did not improve from 2.87903\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2054 - mae: 1.2157 - val_loss: 2.8609 - val_mae: 1.1885\n\nEpoch 00010: val_loss improved from 2.87903 to 2.86087, saving model to models/model1.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9998 - mae: 1.1829 - val_loss: 2.8410 - val_mae: 1.1907\n\nEpoch 00011: val_loss improved from 2.86087 to 2.84096, saving model to models/model1.h5\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8934 - mae: 1.1699 - val_loss: 2.7396 - val_mae: 1.1693\n\nEpoch 00012: val_loss improved from 2.84096 to 2.73964, saving model to models/model1.h5\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8138 - mae: 1.1598 - val_loss: 2.9678 - val_mae: 1.2111\n\nEpoch 00013: val_loss did not improve from 2.73964\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7116 - mae: 1.1443 - val_loss: 2.9421 - val_mae: 1.2093\n\nEpoch 00014: val_loss did not improve from 2.73964\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5243 - mae: 1.1103 - val_loss: 2.7123 - val_mae: 1.1648\n\nEpoch 00015: val_loss improved from 2.73964 to 2.71225, saving model to models/model1.h5\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6110 - mae: 1.1243 - val_loss: 2.7276 - val_mae: 1.1681\n\nEpoch 00016: val_loss did not improve from 2.71225\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4318 - mae: 1.0787 - val_loss: 2.7283 - val_mae: 1.1820\n\nEpoch 00017: val_loss did not improve from 2.71225\nEpoch 18/200\n322/322 [==============================] - 2s 7ms/step - loss: 2.3888 - mae: 1.0902 - val_loss: 2.8132 - val_mae: 1.1910\n\nEpoch 00018: val_loss did not improve from 2.71225\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2669 - mae: 1.0465 - val_loss: 2.6365 - val_mae: 1.1691\n\nEpoch 00019: val_loss improved from 2.71225 to 2.63646, saving model to models/model1.h5\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3010 - mae: 1.0668 - val_loss: 2.7073 - val_mae: 1.1724\n\nEpoch 00020: val_loss did not improve from 2.63646\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2937 - mae: 1.0496 - val_loss: 2.5358 - val_mae: 1.1317\n\nEpoch 00021: val_loss improved from 2.63646 to 2.53577, saving model to models/model1.h5\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1839 - mae: 1.0274 - val_loss: 2.5800 - val_mae: 1.1327\n\nEpoch 00022: val_loss did not improve from 2.53577\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4410 - mae: 1.0751 - val_loss: 2.7872 - val_mae: 1.1719\n\nEpoch 00023: val_loss did not improve from 2.53577\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0451 - mae: 0.9975 - val_loss: 2.7346 - val_mae: 1.1677\n\nEpoch 00024: val_loss did not improve from 2.53577\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0565 - mae: 1.0033 - val_loss: 2.6426 - val_mae: 1.1520\n\nEpoch 00025: val_loss did not improve from 2.53577\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9392 - mae: 0.9890 - val_loss: 2.8489 - val_mae: 1.1899\n\nEpoch 00026: val_loss did not improve from 2.53577\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0281 - mae: 1.0078 - val_loss: 2.5934 - val_mae: 1.1555\n\nEpoch 00027: val_loss did not improve from 2.53577\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0236 - mae: 0.9961 - val_loss: 2.8880 - val_mae: 1.2049\n\nEpoch 00028: val_loss did not improve from 2.53577\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8755 - mae: 0.9829 - val_loss: 2.6653 - val_mae: 1.1645\n\nEpoch 00029: val_loss did not improve from 2.53577\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7258 - mae: 0.9499 - val_loss: 2.6192 - val_mae: 1.1550\n\nEpoch 00030: val_loss did not improve from 2.53577\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1594 - mae: 1.0169 - val_loss: 2.5981 - val_mae: 1.1472\n\nEpoch 00031: val_loss did not improve from 2.53577\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6890 - mae: 0.9321 - val_loss: 2.5784 - val_mae: 1.1344\n\nEpoch 00032: val_loss did not improve from 2.53577\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5613 - mae: 0.9052 - val_loss: 2.6312 - val_mae: 1.1459\n\nEpoch 00033: val_loss did not improve from 2.53577\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5656 - mae: 0.9010 - val_loss: 2.6231 - val_mae: 1.1462\n\nEpoch 00034: val_loss did not improve from 2.53577\nEpoch 35/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.5921 - mae: 0.8797 - val_loss: 2.6382 - val_mae: 1.1433\n\nEpoch 00035: val_loss did not improve from 2.53577\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4254 - mae: 0.8684 - val_loss: 2.5754 - val_mae: 1.1353\n\nEpoch 00036: val_loss did not improve from 2.53577\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4797 - mae: 0.8666 - val_loss: 2.6154 - val_mae: 1.1429\n\nEpoch 00037: val_loss did not improve from 2.53577\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4343 - mae: 0.8518 - val_loss: 2.6410 - val_mae: 1.1418\n\nEpoch 00038: val_loss did not improve from 2.53577\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3163 - mae: 0.8245 - val_loss: 2.5987 - val_mae: 1.1345\n\nEpoch 00039: val_loss did not improve from 2.53577\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4226 - mae: 0.8319 - val_loss: 2.6153 - val_mae: 1.1297\n\nEpoch 00040: val_loss did not improve from 2.53577\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3876 - mae: 0.8265 - val_loss: 2.6033 - val_mae: 1.1407\n","output_type":"stream"},{"name":"stderr","text":"Folds:  40%|████      | 2/5 [04:30<06:17, 125.94s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00041: val_loss did not improve from 2.53577\nEpoch 1/200\n322/322 [==============================] - 3s 7ms/step - loss: 4.2115 - mae: 1.4242 - val_loss: 4.7179 - val_mae: 1.4518\n\nEpoch 00001: val_loss improved from inf to 4.71793, saving model to models/model2.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 4.0276 - mae: 1.3974 - val_loss: 4.6846 - val_mae: 1.4458\n\nEpoch 00002: val_loss improved from 4.71793 to 4.68458, saving model to models/model2.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7509 - mae: 1.3476 - val_loss: 4.3576 - val_mae: 1.3865\n\nEpoch 00003: val_loss improved from 4.68458 to 4.35756, saving model to models/model2.h5\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.6207 - mae: 1.3032 - val_loss: 4.0395 - val_mae: 1.3212\n\nEpoch 00004: val_loss improved from 4.35756 to 4.03948, saving model to models/model2.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.5465 - mae: 1.2997 - val_loss: 4.1174 - val_mae: 1.3269\n\nEpoch 00005: val_loss did not improve from 4.03948\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3931 - mae: 1.2692 - val_loss: 4.0315 - val_mae: 1.3097\n\nEpoch 00006: val_loss improved from 4.03948 to 4.03147, saving model to models/model2.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2860 - mae: 1.2490 - val_loss: 4.1450 - val_mae: 1.3537\n\nEpoch 00007: val_loss did not improve from 4.03147\nEpoch 8/200\n322/322 [==============================] - 2s 7ms/step - loss: 3.2408 - mae: 1.2390 - val_loss: 3.8080 - val_mae: 1.2745\n\nEpoch 00008: val_loss improved from 4.03147 to 3.80795, saving model to models/model2.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1959 - mae: 1.2243 - val_loss: 3.7555 - val_mae: 1.2635\n\nEpoch 00009: val_loss improved from 3.80795 to 3.75552, saving model to models/model2.h5\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9605 - mae: 1.1890 - val_loss: 3.6546 - val_mae: 1.2530\n\nEpoch 00010: val_loss improved from 3.75552 to 3.65465, saving model to models/model2.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8438 - mae: 1.1875 - val_loss: 3.6271 - val_mae: 1.2510\n\nEpoch 00011: val_loss improved from 3.65465 to 3.62707, saving model to models/model2.h5\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8396 - mae: 1.1678 - val_loss: 3.5530 - val_mae: 1.2410\n\nEpoch 00012: val_loss improved from 3.62707 to 3.55296, saving model to models/model2.h5\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8069 - mae: 1.1660 - val_loss: 3.5694 - val_mae: 1.2348\n\nEpoch 00013: val_loss did not improve from 3.55296\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5983 - mae: 1.1278 - val_loss: 3.2587 - val_mae: 1.2007\n\nEpoch 00014: val_loss improved from 3.55296 to 3.25871, saving model to models/model2.h5\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4445 - mae: 1.1115 - val_loss: 3.4977 - val_mae: 1.2584\n\nEpoch 00015: val_loss did not improve from 3.25871\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4370 - mae: 1.1159 - val_loss: 3.1350 - val_mae: 1.1825\n\nEpoch 00016: val_loss improved from 3.25871 to 3.13503, saving model to models/model2.h5\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2741 - mae: 1.0837 - val_loss: 3.3138 - val_mae: 1.2167\n\nEpoch 00017: val_loss did not improve from 3.13503\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3855 - mae: 1.0966 - val_loss: 3.1714 - val_mae: 1.1998\n\nEpoch 00018: val_loss did not improve from 3.13503\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0839 - mae: 1.0407 - val_loss: 3.0769 - val_mae: 1.1897\n\nEpoch 00019: val_loss improved from 3.13503 to 3.07690, saving model to models/model2.h5\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2290 - mae: 1.0724 - val_loss: 3.0083 - val_mae: 1.1617\n\nEpoch 00020: val_loss improved from 3.07690 to 3.00829, saving model to models/model2.h5\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0751 - mae: 1.0150 - val_loss: 2.9630 - val_mae: 1.1513\n\nEpoch 00021: val_loss improved from 3.00829 to 2.96297, saving model to models/model2.h5\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8872 - mae: 0.9932 - val_loss: 3.0506 - val_mae: 1.1669\n\nEpoch 00022: val_loss did not improve from 2.96297\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9244 - mae: 1.0108 - val_loss: 3.0885 - val_mae: 1.1519\n\nEpoch 00023: val_loss did not improve from 2.96297\nEpoch 24/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8600 - mae: 0.9959 - val_loss: 3.0800 - val_mae: 1.1716\n\nEpoch 00024: val_loss did not improve from 2.96297\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0413 - mae: 1.0265 - val_loss: 3.1880 - val_mae: 1.1732\n\nEpoch 00025: val_loss did not improve from 2.96297\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9188 - mae: 0.9946 - val_loss: 3.2422 - val_mae: 1.2108\n\nEpoch 00026: val_loss did not improve from 2.96297\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7757 - mae: 0.9636 - val_loss: 2.9041 - val_mae: 1.1457\n\nEpoch 00027: val_loss improved from 2.96297 to 2.90411, saving model to models/model2.h5\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8279 - mae: 0.9766 - val_loss: 3.3087 - val_mae: 1.1963\n\nEpoch 00028: val_loss did not improve from 2.90411\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8271 - mae: 0.9678 - val_loss: 2.9824 - val_mae: 1.1570\n\nEpoch 00029: val_loss did not improve from 2.90411\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6310 - mae: 0.9409 - val_loss: 3.2264 - val_mae: 1.2081\n\nEpoch 00030: val_loss did not improve from 2.90411\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7094 - mae: 0.9465 - val_loss: 2.9901 - val_mae: 1.1633\n\nEpoch 00031: val_loss did not improve from 2.90411\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5974 - mae: 0.9352 - val_loss: 3.1238 - val_mae: 1.1903\n\nEpoch 00032: val_loss did not improve from 2.90411\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6826 - mae: 0.9382 - val_loss: 3.0928 - val_mae: 1.1830\n\nEpoch 00033: val_loss did not improve from 2.90411\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6520 - mae: 0.9480 - val_loss: 3.1567 - val_mae: 1.1937\n\nEpoch 00034: val_loss did not improve from 2.90411\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6140 - mae: 0.9147 - val_loss: 2.9700 - val_mae: 1.1552\n\nEpoch 00035: val_loss did not improve from 2.90411\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5491 - mae: 0.8969 - val_loss: 2.9077 - val_mae: 1.1426\n\nEpoch 00036: val_loss did not improve from 2.90411\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5311 - mae: 0.9009 - val_loss: 3.0510 - val_mae: 1.1628\n\nEpoch 00037: val_loss did not improve from 2.90411\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4903 - mae: 0.8788 - val_loss: 2.9944 - val_mae: 1.1596\n\nEpoch 00038: val_loss did not improve from 2.90411\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3712 - mae: 0.8576 - val_loss: 2.9345 - val_mae: 1.1454\n\nEpoch 00039: val_loss did not improve from 2.90411\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4069 - mae: 0.8469 - val_loss: 2.9344 - val_mae: 1.1436\n\nEpoch 00040: val_loss did not improve from 2.90411\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3892 - mae: 0.8417 - val_loss: 2.9472 - val_mae: 1.1373\n\nEpoch 00041: val_loss did not improve from 2.90411\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2892 - mae: 0.8265 - val_loss: 2.8705 - val_mae: 1.1329\n\nEpoch 00042: val_loss improved from 2.90411 to 2.87049, saving model to models/model2.h5\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2069 - mae: 0.7925 - val_loss: 2.8952 - val_mae: 1.1315\n\nEpoch 00043: val_loss did not improve from 2.87049\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2124 - mae: 0.7984 - val_loss: 2.9408 - val_mae: 1.1388\n\nEpoch 00044: val_loss did not improve from 2.87049\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2909 - mae: 0.8049 - val_loss: 2.8406 - val_mae: 1.1316\n\nEpoch 00045: val_loss improved from 2.87049 to 2.84059, saving model to models/model2.h5\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3045 - mae: 0.8148 - val_loss: 2.8218 - val_mae: 1.1191\n\nEpoch 00046: val_loss improved from 2.84059 to 2.82176, saving model to models/model2.h5\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3039 - mae: 0.8293 - val_loss: 2.8669 - val_mae: 1.1296\n\nEpoch 00047: val_loss did not improve from 2.82176\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1813 - mae: 0.7971 - val_loss: 2.9321 - val_mae: 1.1452\n\nEpoch 00048: val_loss did not improve from 2.82176\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1895 - mae: 0.7763 - val_loss: 2.8780 - val_mae: 1.1355\n\nEpoch 00049: val_loss did not improve from 2.82176\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1705 - mae: 0.7691 - val_loss: 2.7948 - val_mae: 1.1112\n\nEpoch 00050: val_loss improved from 2.82176 to 2.79475, saving model to models/model2.h5\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0551 - mae: 0.7579 - val_loss: 2.8104 - val_mae: 1.1227\n\nEpoch 00051: val_loss did not improve from 2.79475\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1798 - mae: 0.7697 - val_loss: 2.8199 - val_mae: 1.1184\n\nEpoch 00052: val_loss did not improve from 2.79475\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1365 - mae: 0.7599 - val_loss: 2.8099 - val_mae: 1.1258\n\nEpoch 00053: val_loss did not improve from 2.79475\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1145 - mae: 0.7712 - val_loss: 2.7369 - val_mae: 1.1031\n\nEpoch 00054: val_loss improved from 2.79475 to 2.73690, saving model to models/model2.h5\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0579 - mae: 0.7563 - val_loss: 2.7627 - val_mae: 1.1052\n\nEpoch 00055: val_loss did not improve from 2.73690\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1076 - mae: 0.7654 - val_loss: 2.8716 - val_mae: 1.1224\n\nEpoch 00056: val_loss did not improve from 2.73690\nEpoch 57/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.2279 - mae: 0.7894 - val_loss: 2.9005 - val_mae: 1.1212\n\nEpoch 00057: val_loss did not improve from 2.73690\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1400 - mae: 0.7588 - val_loss: 2.8402 - val_mae: 1.1080\n\nEpoch 00058: val_loss did not improve from 2.73690\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1084 - mae: 0.7554 - val_loss: 2.8012 - val_mae: 1.1068\n\nEpoch 00059: val_loss did not improve from 2.73690\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1221 - mae: 0.7440 - val_loss: 2.8123 - val_mae: 1.1108\n\nEpoch 00060: val_loss did not improve from 2.73690\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1130 - mae: 0.7568 - val_loss: 2.7725 - val_mae: 1.1094\n\nEpoch 00061: val_loss did not improve from 2.73690\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0744 - mae: 0.7467 - val_loss: 2.8219 - val_mae: 1.1258\n\nEpoch 00062: val_loss did not improve from 2.73690\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1168 - mae: 0.7455 - val_loss: 2.7617 - val_mae: 1.1059\n\nEpoch 00063: val_loss did not improve from 2.73690\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0981 - mae: 0.7431 - val_loss: 2.7164 - val_mae: 1.0973\n\nEpoch 00064: val_loss improved from 2.73690 to 2.71644, saving model to models/model2.h5\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2077 - mae: 0.7569 - val_loss: 2.7118 - val_mae: 1.0904\n\nEpoch 00065: val_loss improved from 2.71644 to 2.71185, saving model to models/model2.h5\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0384 - mae: 0.7364 - val_loss: 2.7068 - val_mae: 1.0945\n\nEpoch 00066: val_loss improved from 2.71185 to 2.70677, saving model to models/model2.h5\nEpoch 67/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1482 - mae: 0.7520 - val_loss: 2.8730 - val_mae: 1.1203\n\nEpoch 00067: val_loss did not improve from 2.70677\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0735 - mae: 0.7379 - val_loss: 2.8411 - val_mae: 1.1161\n\nEpoch 00068: val_loss did not improve from 2.70677\nEpoch 69/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0134 - mae: 0.7235 - val_loss: 2.9203 - val_mae: 1.1430\n\nEpoch 00069: val_loss did not improve from 2.70677\nEpoch 70/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0882 - mae: 0.7505 - val_loss: 2.7976 - val_mae: 1.1135\n\nEpoch 00070: val_loss did not improve from 2.70677\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1216 - mae: 0.7529 - val_loss: 2.7716 - val_mae: 1.1101\n\nEpoch 00071: val_loss did not improve from 2.70677\nEpoch 72/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0681 - mae: 0.7252 - val_loss: 2.7890 - val_mae: 1.1162\n\nEpoch 00072: val_loss did not improve from 2.70677\nEpoch 73/200\n322/322 [==============================] - 2s 8ms/step - loss: 1.0177 - mae: 0.7184 - val_loss: 2.7111 - val_mae: 1.0950\n\nEpoch 00073: val_loss did not improve from 2.70677\nEpoch 74/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0618 - mae: 0.7281 - val_loss: 2.8288 - val_mae: 1.1167\n\nEpoch 00074: val_loss did not improve from 2.70677\nEpoch 75/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0560 - mae: 0.7327 - val_loss: 2.7885 - val_mae: 1.1139\n\nEpoch 00075: val_loss did not improve from 2.70677\nEpoch 76/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9749 - mae: 0.7115 - val_loss: 2.7723 - val_mae: 1.1064\n\nEpoch 00076: val_loss did not improve from 2.70677\nEpoch 77/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9683 - mae: 0.7127 - val_loss: 2.7980 - val_mae: 1.1075\n\nEpoch 00077: val_loss did not improve from 2.70677\nEpoch 78/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9498 - mae: 0.7084 - val_loss: 2.7527 - val_mae: 1.1003\n\nEpoch 00078: val_loss did not improve from 2.70677\nEpoch 79/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9642 - mae: 0.7028 - val_loss: 2.7652 - val_mae: 1.1052\n\nEpoch 00079: val_loss did not improve from 2.70677\nEpoch 80/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9721 - mae: 0.6966 - val_loss: 2.8078 - val_mae: 1.1069\n\nEpoch 00080: val_loss did not improve from 2.70677\nEpoch 81/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9575 - mae: 0.6979 - val_loss: 2.7677 - val_mae: 1.0997\n\nEpoch 00081: val_loss did not improve from 2.70677\nEpoch 82/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9283 - mae: 0.6951 - val_loss: 2.7878 - val_mae: 1.1035\n\nEpoch 00082: val_loss did not improve from 2.70677\nEpoch 83/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8727 - mae: 0.6660 - val_loss: 2.7831 - val_mae: 1.1033\n\nEpoch 00083: val_loss did not improve from 2.70677\nEpoch 84/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9624 - mae: 0.6933 - val_loss: 2.7692 - val_mae: 1.1046\n\nEpoch 00084: val_loss did not improve from 2.70677\nEpoch 85/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9618 - mae: 0.6922 - val_loss: 2.7867 - val_mae: 1.1059\n\nEpoch 00085: val_loss did not improve from 2.70677\nEpoch 86/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8565 - mae: 0.6789 - val_loss: 2.7754 - val_mae: 1.1043\n","output_type":"stream"},{"name":"stderr","text":"Folds:  60%|██████    | 3/5 [07:20<04:52, 146.06s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00086: val_loss did not improve from 2.70677\nEpoch 1/200\n322/322 [==============================] - 3s 8ms/step - loss: 4.3434 - mae: 1.4536 - val_loss: 3.9928 - val_mae: 1.3287\n\nEpoch 00001: val_loss improved from inf to 3.99279, saving model to models/model3.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 4.0380 - mae: 1.3893 - val_loss: 3.9072 - val_mae: 1.2860\n\nEpoch 00002: val_loss improved from 3.99279 to 3.90724, saving model to models/model3.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.9091 - mae: 1.3594 - val_loss: 3.9188 - val_mae: 1.2947\n\nEpoch 00003: val_loss did not improve from 3.90724\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.8085 - mae: 1.3345 - val_loss: 3.6139 - val_mae: 1.3005\n\nEpoch 00004: val_loss improved from 3.90724 to 3.61391, saving model to models/model3.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7281 - mae: 1.3157 - val_loss: 3.8386 - val_mae: 1.2737\n\nEpoch 00005: val_loss did not improve from 3.61391\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4570 - mae: 1.2720 - val_loss: 3.3372 - val_mae: 1.1757\n\nEpoch 00006: val_loss improved from 3.61391 to 3.33716, saving model to models/model3.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3502 - mae: 1.2521 - val_loss: 3.1735 - val_mae: 1.1616\n\nEpoch 00007: val_loss improved from 3.33716 to 3.17345, saving model to models/model3.h5\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1179 - mae: 1.2213 - val_loss: 3.1231 - val_mae: 1.1421\n\nEpoch 00008: val_loss improved from 3.17345 to 3.12314, saving model to models/model3.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9452 - mae: 1.1936 - val_loss: 3.2197 - val_mae: 1.1597\n\nEpoch 00009: val_loss did not improve from 3.12314\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6812 - mae: 1.1411 - val_loss: 3.0839 - val_mae: 1.1483\n\nEpoch 00010: val_loss improved from 3.12314 to 3.08393, saving model to models/model3.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6110 - mae: 1.1247 - val_loss: 3.0528 - val_mae: 1.1527\n\nEpoch 00011: val_loss improved from 3.08393 to 3.05282, saving model to models/model3.h5\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4825 - mae: 1.1113 - val_loss: 3.0546 - val_mae: 1.1537\n\nEpoch 00012: val_loss did not improve from 3.05282\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1826 - mae: 1.0585 - val_loss: 2.9804 - val_mae: 1.1339\n\nEpoch 00013: val_loss improved from 3.05282 to 2.98038, saving model to models/model3.h5\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1745 - mae: 1.0578 - val_loss: 3.1231 - val_mae: 1.1632\n\nEpoch 00014: val_loss did not improve from 2.98038\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1827 - mae: 1.0596 - val_loss: 3.0300 - val_mae: 1.1377\n\nEpoch 00015: val_loss did not improve from 2.98038\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9501 - mae: 1.0157 - val_loss: 3.0984 - val_mae: 1.1486\n\nEpoch 00016: val_loss did not improve from 2.98038\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0282 - mae: 1.0288 - val_loss: 2.8740 - val_mae: 1.1343\n\nEpoch 00017: val_loss improved from 2.98038 to 2.87399, saving model to models/model3.h5\nEpoch 18/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8829 - mae: 0.9909 - val_loss: 2.7852 - val_mae: 1.1072\n\nEpoch 00018: val_loss improved from 2.87399 to 2.78524, saving model to models/model3.h5\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9734 - mae: 1.0130 - val_loss: 2.8655 - val_mae: 1.0990\n\nEpoch 00019: val_loss did not improve from 2.78524\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7839 - mae: 0.9657 - val_loss: 2.9201 - val_mae: 1.1090\n\nEpoch 00020: val_loss did not improve from 2.78524\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7613 - mae: 0.9644 - val_loss: 2.8487 - val_mae: 1.1097\n\nEpoch 00021: val_loss did not improve from 2.78524\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8786 - mae: 0.9805 - val_loss: 2.9697 - val_mae: 1.1436\n\nEpoch 00022: val_loss did not improve from 2.78524\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9568 - mae: 1.0045 - val_loss: 3.0280 - val_mae: 1.1349\n\nEpoch 00023: val_loss did not improve from 2.78524\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6893 - mae: 0.9397 - val_loss: 2.7989 - val_mae: 1.0910\n\nEpoch 00024: val_loss did not improve from 2.78524\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6545 - mae: 0.9181 - val_loss: 2.8561 - val_mae: 1.1033\n\nEpoch 00025: val_loss did not improve from 2.78524\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5067 - mae: 0.9130 - val_loss: 2.8228 - val_mae: 1.0847\n\nEpoch 00026: val_loss did not improve from 2.78524\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5488 - mae: 0.9070 - val_loss: 2.8223 - val_mae: 1.0948\n\nEpoch 00027: val_loss did not improve from 2.78524\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7679 - mae: 0.9300 - val_loss: 3.1384 - val_mae: 1.1563\n\nEpoch 00028: val_loss did not improve from 2.78524\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5950 - mae: 0.9128 - val_loss: 2.8518 - val_mae: 1.0975\n\nEpoch 00029: val_loss did not improve from 2.78524\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4125 - mae: 0.8584 - val_loss: 2.7840 - val_mae: 1.0809\n\nEpoch 00030: val_loss improved from 2.78524 to 2.78399, saving model to models/model3.h5\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2906 - mae: 0.8182 - val_loss: 2.8149 - val_mae: 1.0964\n\nEpoch 00031: val_loss did not improve from 2.78399\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3583 - mae: 0.8119 - val_loss: 2.8211 - val_mae: 1.0922\n\nEpoch 00032: val_loss did not improve from 2.78399\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3491 - mae: 0.8255 - val_loss: 2.8358 - val_mae: 1.0915\n\nEpoch 00033: val_loss did not improve from 2.78399\nEpoch 34/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.2786 - mae: 0.8098 - val_loss: 2.8675 - val_mae: 1.0901\n\nEpoch 00034: val_loss did not improve from 2.78399\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2439 - mae: 0.7935 - val_loss: 2.8566 - val_mae: 1.0843\n\nEpoch 00035: val_loss did not improve from 2.78399\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2253 - mae: 0.7804 - val_loss: 2.8192 - val_mae: 1.0870\n\nEpoch 00036: val_loss did not improve from 2.78399\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1489 - mae: 0.7740 - val_loss: 2.8271 - val_mae: 1.0875\n\nEpoch 00037: val_loss did not improve from 2.78399\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1907 - mae: 0.7808 - val_loss: 2.8625 - val_mae: 1.0878\n\nEpoch 00038: val_loss did not improve from 2.78399\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1115 - mae: 0.7744 - val_loss: 2.8592 - val_mae: 1.0841\n\nEpoch 00039: val_loss did not improve from 2.78399\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2193 - mae: 0.7852 - val_loss: 2.8353 - val_mae: 1.0800\n\nEpoch 00040: val_loss did not improve from 2.78399\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0692 - mae: 0.7450 - val_loss: 2.7759 - val_mae: 1.0727\n\nEpoch 00041: val_loss improved from 2.78399 to 2.77593, saving model to models/model3.h5\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0940 - mae: 0.7366 - val_loss: 2.7917 - val_mae: 1.0812\n\nEpoch 00042: val_loss did not improve from 2.77593\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1441 - mae: 0.7355 - val_loss: 2.7546 - val_mae: 1.0733\n\nEpoch 00043: val_loss improved from 2.77593 to 2.75464, saving model to models/model3.h5\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0419 - mae: 0.7539 - val_loss: 2.7638 - val_mae: 1.0728\n\nEpoch 00044: val_loss did not improve from 2.75464\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0259 - mae: 0.7299 - val_loss: 2.7502 - val_mae: 1.0708\n\nEpoch 00045: val_loss improved from 2.75464 to 2.75020, saving model to models/model3.h5\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9516 - mae: 0.7077 - val_loss: 2.7500 - val_mae: 1.0699\n\nEpoch 00046: val_loss improved from 2.75020 to 2.75000, saving model to models/model3.h5\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0836 - mae: 0.7471 - val_loss: 2.7846 - val_mae: 1.0766\n\nEpoch 00047: val_loss did not improve from 2.75000\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0545 - mae: 0.7159 - val_loss: 2.7765 - val_mae: 1.0785\n\nEpoch 00048: val_loss did not improve from 2.75000\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0708 - mae: 0.7295 - val_loss: 2.7670 - val_mae: 1.0781\n\nEpoch 00049: val_loss did not improve from 2.75000\nEpoch 50/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.1128 - mae: 0.7336 - val_loss: 2.7746 - val_mae: 1.0822\n\nEpoch 00050: val_loss did not improve from 2.75000\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0761 - mae: 0.7314 - val_loss: 2.7496 - val_mae: 1.0718\n\nEpoch 00051: val_loss improved from 2.75000 to 2.74958, saving model to models/model3.h5\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0520 - mae: 0.7302 - val_loss: 2.7512 - val_mae: 1.0707\n\nEpoch 00052: val_loss did not improve from 2.74958\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0395 - mae: 0.7372 - val_loss: 2.7641 - val_mae: 1.0730\n\nEpoch 00053: val_loss did not improve from 2.74958\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9874 - mae: 0.7183 - val_loss: 2.7827 - val_mae: 1.0792\n\nEpoch 00054: val_loss did not improve from 2.74958\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9757 - mae: 0.7100 - val_loss: 2.7501 - val_mae: 1.0768\n\nEpoch 00055: val_loss did not improve from 2.74958\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9973 - mae: 0.7072 - val_loss: 2.7606 - val_mae: 1.0741\n\nEpoch 00056: val_loss did not improve from 2.74958\nEpoch 57/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0046 - mae: 0.7172 - val_loss: 2.7747 - val_mae: 1.0799\n\nEpoch 00057: val_loss did not improve from 2.74958\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0438 - mae: 0.7253 - val_loss: 2.7944 - val_mae: 1.0837\n\nEpoch 00058: val_loss did not improve from 2.74958\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0176 - mae: 0.7214 - val_loss: 2.7790 - val_mae: 1.0816\n\nEpoch 00059: val_loss did not improve from 2.74958\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9400 - mae: 0.7171 - val_loss: 2.7976 - val_mae: 1.0850\n\nEpoch 00060: val_loss did not improve from 2.74958\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0122 - mae: 0.7018 - val_loss: 2.7876 - val_mae: 1.0785\n\nEpoch 00061: val_loss did not improve from 2.74958\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9421 - mae: 0.7029 - val_loss: 2.7743 - val_mae: 1.0757\n\nEpoch 00062: val_loss did not improve from 2.74958\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0270 - mae: 0.7002 - val_loss: 2.7815 - val_mae: 1.0786\n\nEpoch 00063: val_loss did not improve from 2.74958\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9419 - mae: 0.6840 - val_loss: 2.7826 - val_mae: 1.0793\n\nEpoch 00064: val_loss did not improve from 2.74958\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9662 - mae: 0.6977 - val_loss: 2.7853 - val_mae: 1.0777\n\nEpoch 00065: val_loss did not improve from 2.74958\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9868 - mae: 0.6903 - val_loss: 2.7688 - val_mae: 1.0766\n\nEpoch 00066: val_loss did not improve from 2.74958\nEpoch 67/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9300 - mae: 0.6992 - val_loss: 2.7811 - val_mae: 1.0775\n\nEpoch 00067: val_loss did not improve from 2.74958\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0569 - mae: 0.7162 - val_loss: 2.7808 - val_mae: 1.0776\n\nEpoch 00068: val_loss did not improve from 2.74958\nEpoch 69/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9476 - mae: 0.6956 - val_loss: 2.7663 - val_mae: 1.0750\n\nEpoch 00069: val_loss did not improve from 2.74958\nEpoch 70/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8789 - mae: 0.6902 - val_loss: 2.7601 - val_mae: 1.0743\n\nEpoch 00070: val_loss did not improve from 2.74958\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8847 - mae: 0.6822 - val_loss: 2.7609 - val_mae: 1.0743\n\nEpoch 00071: val_loss did not improve from 2.74958\n","output_type":"stream"},{"name":"stderr","text":"Folds:  80%|████████  | 4/5 [09:45<02:25, 145.69s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n322/322 [==============================] - 3s 8ms/step - loss: 4.3343 - mae: 1.4393 - val_loss: 3.6303 - val_mae: 1.3483\n\nEpoch 00001: val_loss improved from inf to 3.63028, saving model to models/model4.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 4.1009 - mae: 1.3870 - val_loss: 2.9521 - val_mae: 1.2238\n\nEpoch 00002: val_loss improved from 3.63028 to 2.95209, saving model to models/model4.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.9539 - mae: 1.3467 - val_loss: 3.0782 - val_mae: 1.2468\n\nEpoch 00003: val_loss did not improve from 2.95209\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7731 - mae: 1.3093 - val_loss: 3.0274 - val_mae: 1.2162\n\nEpoch 00004: val_loss did not improve from 2.95209\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4785 - mae: 1.2570 - val_loss: 2.8101 - val_mae: 1.1697\n\nEpoch 00005: val_loss improved from 2.95209 to 2.81010, saving model to models/model4.h5\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7585 - mae: 1.3229 - val_loss: 2.7059 - val_mae: 1.1457\n\nEpoch 00006: val_loss improved from 2.81010 to 2.70590, saving model to models/model4.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4036 - mae: 1.2361 - val_loss: 2.8689 - val_mae: 1.1985\n\nEpoch 00007: val_loss did not improve from 2.70590\nEpoch 8/200\n322/322 [==============================] - 2s 7ms/step - loss: 3.2863 - mae: 1.2201 - val_loss: 2.7340 - val_mae: 1.1592\n\nEpoch 00008: val_loss did not improve from 2.70590\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.0435 - mae: 1.1687 - val_loss: 2.7604 - val_mae: 1.1671\n\nEpoch 00009: val_loss did not improve from 2.70590\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9503 - mae: 1.1616 - val_loss: 2.8324 - val_mae: 1.1822\n\nEpoch 00010: val_loss did not improve from 2.70590\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8752 - mae: 1.1436 - val_loss: 2.6130 - val_mae: 1.1351\n\nEpoch 00011: val_loss improved from 2.70590 to 2.61298, saving model to models/model4.h5\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7208 - mae: 1.1135 - val_loss: 2.5947 - val_mae: 1.1451\n\nEpoch 00012: val_loss improved from 2.61298 to 2.59472, saving model to models/model4.h5\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5869 - mae: 1.1100 - val_loss: 2.4406 - val_mae: 1.1270\n\nEpoch 00013: val_loss improved from 2.59472 to 2.44060, saving model to models/model4.h5\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5506 - mae: 1.0944 - val_loss: 2.6636 - val_mae: 1.1685\n\nEpoch 00014: val_loss did not improve from 2.44060\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3879 - mae: 1.0828 - val_loss: 2.7664 - val_mae: 1.1930\n\nEpoch 00015: val_loss did not improve from 2.44060\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3435 - mae: 1.0654 - val_loss: 2.6419 - val_mae: 1.1596\n\nEpoch 00016: val_loss did not improve from 2.44060\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1260 - mae: 1.0230 - val_loss: 2.5415 - val_mae: 1.1450\n\nEpoch 00017: val_loss did not improve from 2.44060\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1146 - mae: 1.0345 - val_loss: 2.5290 - val_mae: 1.1241\n\nEpoch 00018: val_loss did not improve from 2.44060\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0793 - mae: 1.0223 - val_loss: 2.5824 - val_mae: 1.1377\n\nEpoch 00019: val_loss did not improve from 2.44060\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1959 - mae: 1.0241 - val_loss: 2.5439 - val_mae: 1.1397\n\nEpoch 00020: val_loss did not improve from 2.44060\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1106 - mae: 1.0147 - val_loss: 2.3953 - val_mae: 1.1103\n\nEpoch 00021: val_loss improved from 2.44060 to 2.39526, saving model to models/model4.h5\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9442 - mae: 0.9748 - val_loss: 2.3973 - val_mae: 1.1018\n\nEpoch 00022: val_loss did not improve from 2.39526\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8774 - mae: 0.9457 - val_loss: 2.4368 - val_mae: 1.1180\n\nEpoch 00023: val_loss did not improve from 2.39526\nEpoch 24/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8945 - mae: 0.9478 - val_loss: 2.3663 - val_mae: 1.0963\n\nEpoch 00024: val_loss improved from 2.39526 to 2.36631, saving model to models/model4.h5\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8154 - mae: 0.9506 - val_loss: 2.4348 - val_mae: 1.1046\n\nEpoch 00025: val_loss did not improve from 2.36631\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8213 - mae: 0.9327 - val_loss: 2.5155 - val_mae: 1.1328\n\nEpoch 00026: val_loss did not improve from 2.36631\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8596 - mae: 0.9633 - val_loss: 2.3498 - val_mae: 1.0950\n\nEpoch 00027: val_loss improved from 2.36631 to 2.34982, saving model to models/model4.h5\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6775 - mae: 0.9246 - val_loss: 2.2912 - val_mae: 1.0636\n\nEpoch 00028: val_loss improved from 2.34982 to 2.29118, saving model to models/model4.h5\nEpoch 29/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8274 - mae: 0.9307 - val_loss: 2.3672 - val_mae: 1.1003\n\nEpoch 00029: val_loss did not improve from 2.29118\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8937 - mae: 0.9310 - val_loss: 2.2770 - val_mae: 1.0833\n\nEpoch 00030: val_loss improved from 2.29118 to 2.27697, saving model to models/model4.h5\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7242 - mae: 0.9244 - val_loss: 2.8140 - val_mae: 1.2015\n\nEpoch 00031: val_loss did not improve from 2.27697\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7784 - mae: 0.9304 - val_loss: 2.4564 - val_mae: 1.1235\n\nEpoch 00032: val_loss did not improve from 2.27697\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5983 - mae: 0.8968 - val_loss: 2.5427 - val_mae: 1.1473\n\nEpoch 00033: val_loss did not improve from 2.27697\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4755 - mae: 0.8838 - val_loss: 2.3992 - val_mae: 1.1038\n\nEpoch 00034: val_loss did not improve from 2.27697\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5836 - mae: 0.9130 - val_loss: 2.3815 - val_mae: 1.1184\n\nEpoch 00035: val_loss did not improve from 2.27697\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4001 - mae: 0.8642 - val_loss: 2.4798 - val_mae: 1.1219\n\nEpoch 00036: val_loss did not improve from 2.27697\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4757 - mae: 0.8636 - val_loss: 2.3395 - val_mae: 1.0935\n\nEpoch 00037: val_loss did not improve from 2.27697\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8736 - mae: 0.9505 - val_loss: 2.2907 - val_mae: 1.0759\n\nEpoch 00038: val_loss did not improve from 2.27697\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5760 - mae: 0.8785 - val_loss: 2.4261 - val_mae: 1.1153\n\nEpoch 00039: val_loss did not improve from 2.27697\nEpoch 40/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.7251 - mae: 0.8907 - val_loss: 2.3342 - val_mae: 1.0997\n\nEpoch 00040: val_loss did not improve from 2.27697\nEpoch 41/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.4493 - mae: 0.8191 - val_loss: 2.3282 - val_mae: 1.0926\n\nEpoch 00041: val_loss did not improve from 2.27697\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3063 - mae: 0.7962 - val_loss: 2.3708 - val_mae: 1.0995\n\nEpoch 00042: val_loss did not improve from 2.27697\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2030 - mae: 0.7757 - val_loss: 2.3344 - val_mae: 1.0959\n\nEpoch 00043: val_loss did not improve from 2.27697\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2157 - mae: 0.7798 - val_loss: 2.3463 - val_mae: 1.0957\n\nEpoch 00044: val_loss did not improve from 2.27697\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2438 - mae: 0.7854 - val_loss: 2.4160 - val_mae: 1.1089\n\nEpoch 00045: val_loss did not improve from 2.27697\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2544 - mae: 0.7723 - val_loss: 2.2902 - val_mae: 1.0844\n\nEpoch 00046: val_loss did not improve from 2.27697\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2328 - mae: 0.7712 - val_loss: 2.2988 - val_mae: 1.0848\n\nEpoch 00047: val_loss did not improve from 2.27697\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0952 - mae: 0.7452 - val_loss: 2.3149 - val_mae: 1.0820\n\nEpoch 00048: val_loss did not improve from 2.27697\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1552 - mae: 0.7514 - val_loss: 2.2958 - val_mae: 1.0828\n\nEpoch 00049: val_loss did not improve from 2.27697\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2567 - mae: 0.7585 - val_loss: 2.3582 - val_mae: 1.0959\n\nEpoch 00050: val_loss did not improve from 2.27697\n","output_type":"stream"},{"name":"stderr","text":"Folds: 100%|██████████| 5/5 [12:10<00:00, 146.10s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"cv = 0.\nfor l in val_losses:\n    plt.plot(l)\n    cv += np.min(l)\n\nplt.title(f\"CV MSE loss: {cv / len(val_losses):.03f}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:17:34.125282Z","iopub.execute_input":"2022-12-30T12:17:34.127637Z","iopub.status.idle":"2022-12-30T12:17:34.360679Z","shell.execute_reply.started":"2022-12-30T12:17:34.127588Z","shell.execute_reply":"2022-12-30T12:17:34.359689Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'CV MSE loss: 2.690')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"import glob\nthermonet_models = [load_model(f) for f in glob.glob(f'{MODELS_PATH}/model*.h5')]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:17:34.362013Z","iopub.execute_input":"2022-12-30T12:17:34.362691Z","iopub.status.idle":"2022-12-30T12:17:34.911617Z","shell.execute_reply.started":"2022-12-30T12:17:34.362651Z","shell.execute_reply":"2022-12-30T12:17:34.910612Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"TEST_FEATURES_PATH = '../input/thermonet-features/nesp_features.npy'\nif not os.path.exists(TEST_FEATURES_PATH):    \n    np.save(thermonet_features(df_test.query('op == \"replace\"')), 'test_features.npy')\n    TEST_FEATURES_PATH = 'test_features.npy'","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:17:34.913635Z","iopub.execute_input":"2022-12-30T12:17:34.914007Z","iopub.status.idle":"2022-12-30T12:17:34.922044Z","shell.execute_reply.started":"2022-12-30T12:17:34.91397Z","shell.execute_reply":"2022-12-30T12:17:34.921003Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"test_features = np.load(TEST_FEATURES_PATH)\ntest_features = np.moveaxis(test_features, 1, -1)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:17:34.923699Z","iopub.execute_input":"2022-12-30T12:17:34.924273Z","iopub.status.idle":"2022-12-30T12:17:42.447084Z","shell.execute_reply.started":"2022-12-30T12:17:34.924229Z","shell.execute_reply":"2022-12-30T12:17:42.4449Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_ddg = np.stack([model.predict(test_features) for model in thermonet_models])\ntest_ddg = np.mean(test_ddg, axis=0).flatten()\ntest_ddg","metadata":{"execution":{"iopub.status.busy":"2022-12-30T12:17:42.448675Z","iopub.execute_input":"2022-12-30T12:17:42.449064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.loc[df_test.op == 'replace', 'ddg'] = -test_ddg\ndf_test.loc[df_test['op'] == \"delete\", 'ddg'] = df_test[df_test[\"op\"]==\"replace\"][\"ddg\"].quantile(q=0.25)\ndf_test.loc[df_test['op'] == \"same\", 'ddg'] = 0.\ndf_test.rename(columns={'ddg': 'tm'})[['seq_id', 'tm']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}