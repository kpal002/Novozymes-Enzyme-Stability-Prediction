{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kuntalpal/few-baselines-without-much-effort?scriptVersionId=115087763\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"%%capture\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T11:31:19.128476Z","iopub.execute_input":"2022-12-30T11:31:19.128925Z","iopub.status.idle":"2022-12-30T11:31:22.251465Z","shell.execute_reply.started":"2022-12-30T11:31:19.128882Z","shell.execute_reply":"2022-12-30T11:31:22.250407Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Physics behind Protein Stability","metadata":{}},{"cell_type":"markdown","source":"**DDG** : Delta Delta G (DDG) is a metric for predicting how a single point mutation will affect protein stability. The thermodynamic stability change upon mutation is measured by ΔΔG(T$_{r}$), i.e. the difference between the standard Gibbs folding free energies of the mutant (ΔG$^{mut}$) and wild-type (ΔG$^{wild}$) proteins at the reference temperature T$_{r}$:\n\n$$  ΔΔG(T_{r}) = (ΔG^{mut})(T_{r}) - (ΔG^{wild})(T_{r}) $$\n\nIn general,\n\n$$ G = Enthalpy \\ \\  (H) - Temperature \\ \\ (T) \\ x \\ Entropy \\ \\ (S)  $$\n\nH is the internal energy of a protein. H decreases during protein folding because folding will cause packing of hydrophobics, optimized polar group orientation, and achieves a good proximity to ideal bond lengths and angles. S is the measure of order within a system. S of a protein becomes lower during protein folding because residue dynamics will be significantly decreased in comparison the unfolded state. However, decrease is in protein S will also cause an increase in S for solvent. The energy of an unfolded protein is nearly identical with a single point mutation. So, the\ndominant factor in DDG is the energy difference of the folded state.\n","metadata":{}},{"cell_type":"markdown","source":"DDG results will fall into three categories:\n\n1. **DDG > 0.5**: Positive results suggest that a mutation would be destabilizing. Most mutations will be positive or close to zero because proteins have evolved to be reasonably stable. These mutations are residues that you should usually avoid during design.\n\n2. Things that are near 0 are within the noise range so should be considered neutral.\n\n3. DDG < -0.5: Negative results suggest that the mutation would lead to a more stable protein. However, the environment at each position should be considered.\n\n    a. If interacting molecules are not present in the model, such as at a known zinc binding site, then a seemingly favorable mutation will not be favorable in reality.\n    \n    b. A positions that has a lot of negative DDGs could mean that this position evolved a destabilizing residue because it is necessary for its catalytic activity, for binding another molecule, or because of another functionally relevant reason.\n    \n    c. Also, consider that this measures a single point mutation. Many times it requires multiple interacting mutations in order to achieve significant stability.","metadata":{}},{"cell_type":"markdown","source":"# ESM-2\n\nESM-2 is a transformer-based language model, and uses an attention mechanism to learn interaction patterns between pairs of amino acids in the input sequence.\n\nThe larger the value of the attention, the more impactful would be any mutation at that location. So the melting temperature is inversely proportional to the sum of the contact attentions.","metadata":{}},{"cell_type":"code","source":"# # latest release\n# !pip install fair-esm","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:31:22.253837Z","iopub.execute_input":"2022-12-30T11:31:22.254214Z","iopub.status.idle":"2022-12-30T11:31:22.259669Z","shell.execute_reply.started":"2022-12-30T11:31:22.254177Z","shell.execute_reply":"2022-12-30T11:31:22.258534Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# #set the edit_idx\n# import pandas as pd, numpy as np, nltk\n# from sklearn.model_selection import GroupKFold\n# import torch\n# import esm\n#wt = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n#df_test = pd.read_csv(\"../input/novozymes-enzyme-stability-prediction/test.csv\")\n#df_test.loc[1169, 'protein_sequence'] = wt[:-1]+\"!\" #1169 is the same as wt\n\n#df_test['edit_idx'] = df_test.apply(lambda x:[i for i in range(len(x['protein_sequence'])) if x['protein_sequence'][i] != wt[i]][0], axis = 1)\n\n\n# # Load ESM-2 model\n# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n# batch_converter = alphabet.get_batch_converter()\n# model.eval()  # disables dropout for deterministic results\n\n\n# # Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n# data = [\n#     (\"protein1\", wt),\n# ]\n# batch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# # Extract per-residue representations (on CPU)\n# with torch.no_grad():\n#     results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n# token_representations = results[\"representations\"][33]\n\n# # Generate per-sequence representations via averaging\n# # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n# sequence_representations = []\n# for i, (_, seq) in enumerate(data):\n#     sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n\n    \n# #all the code above is from ESM QuickStart\n# ac_sum = np.sum(np.array(results[\"contacts\"][0]), axis=1) \n# test_df['tm'] = test_df.apply(lambda x:1/ac_sum[x['edit_idx']], axis = 1)  \n# test_df[['seq_id', 'tm']].set_index(\"seq_id\").to_csv(\"submission.csv\")\n#df_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:52:44.429698Z","iopub.execute_input":"2022-12-30T11:52:44.430466Z","iopub.status.idle":"2022-12-30T11:52:46.358784Z","shell.execute_reply.started":"2022-12-30T11:52:44.430427Z","shell.execute_reply":"2022-12-30T11:52:46.357402Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      seq_id                                   protein_sequence  pH  \\\n0      31390  VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n1      31391  VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2      31392  VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8   \n3      31393  VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n4      31394  VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n...      ...                                                ...  ..   \n2408   33798  VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2409   33799  VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2410   33800  VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2411   33801  VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2412   33802  VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n\n     data_source  edit_idx  \n0      Novozymes        16  \n1      Novozymes        16  \n2      Novozymes        16  \n3      Novozymes        17  \n4      Novozymes        17  \n...          ...       ...  \n2408   Novozymes        15  \n2409   Novozymes        15  \n2410   Novozymes        15  \n2411   Novozymes        15  \n2412   Novozymes        15  \n\n[2413 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seq_id</th>\n      <th>protein_sequence</th>\n      <th>pH</th>\n      <th>data_source</th>\n      <th>edit_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>31390</td>\n      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31391</td>\n      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31392</td>\n      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31393</td>\n      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31394</td>\n      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>33798</td>\n      <td>VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>33799</td>\n      <td>VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>33800</td>\n      <td>VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>33801</td>\n      <td>VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>33802</td>\n      <td>VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/nesp-public-train-sets/Q3214/Q3214_direct.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:31:22.272129Z","iopub.execute_input":"2022-12-30T11:31:22.272666Z","iopub.status.idle":"2022-12-30T11:31:22.317772Z","shell.execute_reply.started":"2022-12-30T11:31:22.272628Z","shell.execute_reply":"2022-12-30T11:31:22.316666Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     pdb_id  position wild_type mutant   ddg\n0     1otrB        34         E      A -0.07\n1     1a5eA       121         L      R -0.66\n2     1rtbA         4         A      S  0.47\n3     4lyzA       102         G      R -0.38\n4     1thqA       157         M      A  0.77\n...     ...       ...       ...    ...   ...\n3209  2lzmA        42         A      K  3.70\n3210  1yeaA        76         P      G  1.20\n3211  1stnA       104         V      T  2.50\n3212  2lzmA        71         V      A  1.50\n3213  1vqbA        86         A      T  0.70\n\n[3214 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pdb_id</th>\n      <th>position</th>\n      <th>wild_type</th>\n      <th>mutant</th>\n      <th>ddg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1otrB</td>\n      <td>34</td>\n      <td>E</td>\n      <td>A</td>\n      <td>-0.07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1a5eA</td>\n      <td>121</td>\n      <td>L</td>\n      <td>R</td>\n      <td>-0.66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1rtbA</td>\n      <td>4</td>\n      <td>A</td>\n      <td>S</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4lyzA</td>\n      <td>102</td>\n      <td>G</td>\n      <td>R</td>\n      <td>-0.38</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1thqA</td>\n      <td>157</td>\n      <td>M</td>\n      <td>A</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3209</th>\n      <td>2lzmA</td>\n      <td>42</td>\n      <td>A</td>\n      <td>K</td>\n      <td>3.70</td>\n    </tr>\n    <tr>\n      <th>3210</th>\n      <td>1yeaA</td>\n      <td>76</td>\n      <td>P</td>\n      <td>G</td>\n      <td>1.20</td>\n    </tr>\n    <tr>\n      <th>3211</th>\n      <td>1stnA</td>\n      <td>104</td>\n      <td>V</td>\n      <td>T</td>\n      <td>2.50</td>\n    </tr>\n    <tr>\n      <th>3212</th>\n      <td>2lzmA</td>\n      <td>71</td>\n      <td>V</td>\n      <td>A</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>3213</th>\n      <td>1vqbA</td>\n      <td>86</td>\n      <td>A</td>\n      <td>T</td>\n      <td>0.70</td>\n    </tr>\n  </tbody>\n</table>\n<p>3214 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def gen_mutations(name, df,\n                  wild=\"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQ\"\"RVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGT\"\"NAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKAL\"\"GSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"):\n    result = []\n    for _, r in df.iterrows():\n        ops = Levenshtein.editops(wild, r.protein_sequence)\n        assert len(ops) <= 1\n        if len(ops) > 0 and ops[0][0] == 'replace':\n            idx = ops[0][1]\n            result.append([ops[0][0], idx + 1, wild[idx], r.protein_sequence[idx]])\n        elif len(ops) == 0:\n            result.append(['same', 0, '', ''])\n        elif ops[0][0] == 'insert':\n            assert False, \"Ups\"\n        elif ops[0][0] == 'delete':\n            idx = ops[0][1]\n            result.append(['delete', idx + 1, wild[idx], '-'])\n        else:\n            assert False, \"Ups\"\n\n    df = pd.concat([df, pd.DataFrame(data=result, columns=['op', 'idx', 'wild', 'mutant'])], axis=1)\n    df['mut'] = df[['wild', 'idx', 'mutant']].astype(str).apply(lambda v: ''.join(v), axis=1)\n    df['name'] = name\n    return df\n\n\ndf_test = gen_mutations('wildtypeA', pd.read_csv('../input/novozymes-enzyme-stability-prediction/test.csv'))\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:56:51.731041Z","iopub.execute_input":"2022-12-30T11:56:51.732084Z","iopub.status.idle":"2022-12-30T11:56:51.937459Z","shell.execute_reply.started":"2022-12-30T11:56:51.732026Z","shell.execute_reply":"2022-12-30T11:56:51.936386Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      seq_id                                   protein_sequence  pH  \\\n0      31390  VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n1      31391  VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2      31392  VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...   8   \n3      31393  VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n4      31394  VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n...      ...                                                ...  ..   \n2408   33798  VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2409   33799  VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2410   33800  VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2411   33801  VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n2412   33802  VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...   8   \n\n     data_source       op  idx wild mutant   mut       name  \n0      Novozymes  replace   17    L      E  L17E  wildtypeA  \n1      Novozymes  replace   17    L      K  L17K  wildtypeA  \n2      Novozymes   delete   17    L      -  L17-  wildtypeA  \n3      Novozymes  replace   18    K      C  K18C  wildtypeA  \n4      Novozymes  replace   18    K      F  K18F  wildtypeA  \n...          ...      ...  ...  ...    ...   ...        ...  \n2408   Novozymes  replace   16    A      I  A16I  wildtypeA  \n2409   Novozymes  replace   16    A      L  A16L  wildtypeA  \n2410   Novozymes  replace   16    A      N  A16N  wildtypeA  \n2411   Novozymes  replace   16    A      P  A16P  wildtypeA  \n2412   Novozymes  replace   16    A      W  A16W  wildtypeA  \n\n[2413 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seq_id</th>\n      <th>protein_sequence</th>\n      <th>pH</th>\n      <th>data_source</th>\n      <th>op</th>\n      <th>idx</th>\n      <th>wild</th>\n      <th>mutant</th>\n      <th>mut</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>31390</td>\n      <td>VPVNPEPDATSVENVAEKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>17</td>\n      <td>L</td>\n      <td>E</td>\n      <td>L17E</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31391</td>\n      <td>VPVNPEPDATSVENVAKKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>17</td>\n      <td>L</td>\n      <td>K</td>\n      <td>L17K</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31392</td>\n      <td>VPVNPEPDATSVENVAKTGSGDSQSDPIKADLEVKGQSALPFDVDC...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>delete</td>\n      <td>17</td>\n      <td>L</td>\n      <td>-</td>\n      <td>L17-</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31393</td>\n      <td>VPVNPEPDATSVENVALCTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>18</td>\n      <td>K</td>\n      <td>C</td>\n      <td>K18C</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31394</td>\n      <td>VPVNPEPDATSVENVALFTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>18</td>\n      <td>K</td>\n      <td>F</td>\n      <td>K18F</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>33798</td>\n      <td>VPVNPEPDATSVENVILKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>I</td>\n      <td>A16I</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>33799</td>\n      <td>VPVNPEPDATSVENVLLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>L</td>\n      <td>A16L</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>33800</td>\n      <td>VPVNPEPDATSVENVNLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>N</td>\n      <td>A16N</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>33801</td>\n      <td>VPVNPEPDATSVENVPLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>P</td>\n      <td>A16P</td>\n      <td>wildtypeA</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>33802</td>\n      <td>VPVNPEPDATSVENVWLKTGSGDSQSDPIKADLEVKGQSALPFDVD...</td>\n      <td>8</td>\n      <td>Novozymes</td>\n      <td>replace</td>\n      <td>16</td>\n      <td>A</td>\n      <td>W</td>\n      <td>A16W</td>\n      <td>wildtypeA</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"TRAIN_FEATURES_PATH = '../input/thermonet-features/Q3214.npy'\nX = np.load(TRAIN_FEATURES_PATH)\nX = np.moveaxis(X, 1, -1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:31:22.32101Z","iopub.execute_input":"2022-12-30T11:31:22.321402Z","iopub.status.idle":"2022-12-30T11:31:31.824242Z","shell.execute_reply.started":"2022-12-30T11:31:22.321362Z","shell.execute_reply":"2022-12-30T11:31:31.823287Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(3214, 16, 16, 16, 14)"},"metadata":{}}]},{"cell_type":"code","source":"pdb_ids = df_train.pdb_id\ny = df_train.ddg","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:31:31.825352Z","iopub.execute_input":"2022-12-30T11:31:31.830486Z","iopub.status.idle":"2022-12-30T11:31:31.83819Z","shell.execute_reply.started":"2022-12-30T11:31:31.830436Z","shell.execute_reply":"2022-12-30T11:31:31.837514Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import math\nimport multiprocessing\nimport os\nimport sys\n\nimport Levenshtein\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport tensorflow as tf\nfrom keras import layers, callbacks\nfrom keras import models\nfrom keras import optimizers\nfrom sklearn.model_selection import GroupKFold\nfrom keras.saving.save import load_model\nfrom tqdm import tqdm\n\ndef gen_model(params):\n    def build_model(params):\n        conv_layer_sizes = params['conv_layer_sizes']\n        dense_layer_size = params['dense_layer_size']\n        dropout_rate = params['dropout_rate']\n        model = models.Sequential()\n        model.add(layers.Conv3D(filters=conv_layer_sizes[0], kernel_size=(3, 3, 3), input_shape=(16, 16, 16, 14)))\n        model.add(layers.Activation(activation='relu'))\n\n        for ls in conv_layer_sizes[1:]:\n            model.add(layers.Conv3D(filters=ls, kernel_size=(3, 3, 3)))\n            model.add(layers.Activation(activation='relu'))\n\n        model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n        model.add(layers.Flatten())\n\n        model.add(layers.Dropout(rate=dropout_rate))\n        model.add(layers.Dense(units=dense_layer_size, activation='relu'))\n        model.add(layers.Dropout(rate=dropout_rate))\n        model.add(layers.Dense(units=1))\n        return model\n\n    model = build_model(params)\n    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(\n        learning_rate=params['learning_rate'],\n        beta_1=0.9,\n        beta_2=0.999,\n        amsgrad=False\n    ), metrics=['mae'])\n    return model\n\n\ndef train_model(X_train, y_train, X_val, y_val, params, path):\n    model = gen_model(params)\n    scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=params['scheduler_factor'],\n        patience=params['scheduler_patience'],\n        verbose=0,\n        mode=\"auto\",\n        min_delta=0.0001,\n        cooldown=0,\n        min_lr=1e-5,\n    )\n\n    checkpoint = callbacks.ModelCheckpoint(path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=params['early_stopping_patience'])\n    result = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                       epochs=EPOCHS, batch_size=params['batch_size'], verbose=1, callbacks=[scheduler, checkpoint, early_stopping])\n    return load_model(path), result\n\nN_FOLDS = 5\nGROUP_KFOLD = False\nEPOCHS = 200\nMODELS_PATH = 'models'\n\n\n\n\nPARAMS = {\n    'conv_layer_sizes': (16, 24, 32),\n    'dense_layer_size': 24,\n    'dropout_rate': 0.5,\n    'learning_rate': 0.001,\n    'batch_size': 8,\n    'scheduler_patience': 10,\n    'scheduler_factor': math.sqrt(0.1),\n    'early_stopping_patience': 20,\n}\n\n\n!mkdir -p models\nkfold = GroupKFold(N_FOLDS)\nthermonet_models = []\nval_losses = []\nif GROUP_KFOLD:\n    groups = pdb_ids\nelse:\n    groups = range(len(pdb_ids))\n\nfor fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(X, y, groups=groups), total=N_FOLDS, desc=\"Folds\")):\n    X_train = X[train_idx]\n    y_train = y[train_idx]\n    X_val = X[val_idx]\n    y_val = y[val_idx]\n    path = f'{MODELS_PATH}/model{fold}.h5'\n    model, result = train_model(X_train, y_train, X_val, y_val, PARAMS, path)\n    thermonet_models.append(model)\n    val_losses.append(result.history['val_loss'])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:33:34.747353Z","iopub.execute_input":"2022-12-30T11:33:34.748041Z","iopub.status.idle":"2022-12-30T11:47:16.203385Z","shell.execute_reply.started":"2022-12-30T11:33:34.747999Z","shell.execute_reply":"2022-12-30T11:47:16.202165Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Folds:   0%|          | 0/5 [00:00<?, ?it/s]2022-12-30 11:33:36.284610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:36.424979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:36.425849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:36.427597: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-30 11:33:36.427971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:36.428682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:36.429318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:38.730862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:38.731781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:38.732479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-30 11:33:38.733146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2022-12-30 11:33:41.082246: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"2022-12-30 11:33:42.512658: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"322/322 [==============================] - 10s 8ms/step - loss: 4.1920 - mae: 1.4280 - val_loss: 4.6030 - val_mae: 1.5663\n\nEpoch 00001: val_loss improved from inf to 4.60298, saving model to models/model0.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 4.0478 - mae: 1.3916 - val_loss: 4.1502 - val_mae: 1.3870\n\nEpoch 00002: val_loss improved from 4.60298 to 4.15017, saving model to models/model0.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7789 - mae: 1.3320 - val_loss: 4.0601 - val_mae: 1.3682\n\nEpoch 00003: val_loss improved from 4.15017 to 4.06010, saving model to models/model0.h5\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.6419 - mae: 1.3058 - val_loss: 3.7806 - val_mae: 1.3263\n\nEpoch 00004: val_loss improved from 4.06010 to 3.78056, saving model to models/model0.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4372 - mae: 1.2658 - val_loss: 3.8682 - val_mae: 1.3162\n\nEpoch 00005: val_loss did not improve from 3.78056\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2231 - mae: 1.2251 - val_loss: 3.8214 - val_mae: 1.3225\n\nEpoch 00006: val_loss did not improve from 3.78056\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1040 - mae: 1.2097 - val_loss: 3.8264 - val_mae: 1.3063\n\nEpoch 00007: val_loss did not improve from 3.78056\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9888 - mae: 1.1943 - val_loss: 3.5738 - val_mae: 1.2845\n\nEpoch 00008: val_loss improved from 3.78056 to 3.57384, saving model to models/model0.h5\nEpoch 9/200\n322/322 [==============================] - 2s 7ms/step - loss: 2.8678 - mae: 1.1667 - val_loss: 3.8133 - val_mae: 1.3024\n\nEpoch 00009: val_loss did not improve from 3.57384\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7073 - mae: 1.1619 - val_loss: 3.7099 - val_mae: 1.3010\n\nEpoch 00010: val_loss did not improve from 3.57384\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6819 - mae: 1.1446 - val_loss: 3.4728 - val_mae: 1.2398\n\nEpoch 00011: val_loss improved from 3.57384 to 3.47284, saving model to models/model0.h5\nEpoch 12/200\n322/322 [==============================] - 2s 7ms/step - loss: 2.5126 - mae: 1.1012 - val_loss: 3.5807 - val_mae: 1.2761\n\nEpoch 00012: val_loss did not improve from 3.47284\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4764 - mae: 1.0993 - val_loss: 3.4806 - val_mae: 1.2399\n\nEpoch 00013: val_loss did not improve from 3.47284\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2304 - mae: 1.0585 - val_loss: 3.6259 - val_mae: 1.2802\n\nEpoch 00014: val_loss did not improve from 3.47284\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2955 - mae: 1.0506 - val_loss: 3.4286 - val_mae: 1.2396\n\nEpoch 00015: val_loss improved from 3.47284 to 3.42858, saving model to models/model0.h5\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2980 - mae: 1.0603 - val_loss: 3.5758 - val_mae: 1.2678\n\nEpoch 00016: val_loss did not improve from 3.42858\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2505 - mae: 1.0364 - val_loss: 3.4980 - val_mae: 1.2370\n\nEpoch 00017: val_loss did not improve from 3.42858\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1509 - mae: 1.0274 - val_loss: 3.5835 - val_mae: 1.2686\n\nEpoch 00018: val_loss did not improve from 3.42858\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2265 - mae: 1.0375 - val_loss: 3.4604 - val_mae: 1.2449\n\nEpoch 00019: val_loss did not improve from 3.42858\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0321 - mae: 1.0094 - val_loss: 3.4060 - val_mae: 1.2186\n\nEpoch 00020: val_loss improved from 3.42858 to 3.40605, saving model to models/model0.h5\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9883 - mae: 0.9840 - val_loss: 3.3140 - val_mae: 1.1979\n\nEpoch 00021: val_loss improved from 3.40605 to 3.31404, saving model to models/model0.h5\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8526 - mae: 0.9779 - val_loss: 3.4728 - val_mae: 1.2127\n\nEpoch 00022: val_loss did not improve from 3.31404\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1292 - mae: 1.0143 - val_loss: 3.5444 - val_mae: 1.2621\n\nEpoch 00023: val_loss did not improve from 3.31404\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8206 - mae: 0.9677 - val_loss: 3.3681 - val_mae: 1.2173\n\nEpoch 00024: val_loss did not improve from 3.31404\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8272 - mae: 0.9546 - val_loss: 3.3628 - val_mae: 1.2077\n\nEpoch 00025: val_loss did not improve from 3.31404\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7960 - mae: 0.9654 - val_loss: 3.3822 - val_mae: 1.2072\n\nEpoch 00026: val_loss did not improve from 3.31404\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7966 - mae: 0.9616 - val_loss: 3.4636 - val_mae: 1.2109\n\nEpoch 00027: val_loss did not improve from 3.31404\nEpoch 28/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.7747 - mae: 0.9520 - val_loss: 3.4170 - val_mae: 1.2109\n\nEpoch 00028: val_loss did not improve from 3.31404\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7729 - mae: 0.9379 - val_loss: 3.5126 - val_mae: 1.2405\n\nEpoch 00029: val_loss did not improve from 3.31404\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7149 - mae: 0.9323 - val_loss: 3.4092 - val_mae: 1.2193\n\nEpoch 00030: val_loss did not improve from 3.31404\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6357 - mae: 0.9118 - val_loss: 3.4743 - val_mae: 1.2341\n\nEpoch 00031: val_loss did not improve from 3.31404\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5456 - mae: 0.8697 - val_loss: 3.3154 - val_mae: 1.1961\n\nEpoch 00032: val_loss did not improve from 3.31404\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4605 - mae: 0.8525 - val_loss: 3.2861 - val_mae: 1.1958\n\nEpoch 00033: val_loss improved from 3.31404 to 3.28614, saving model to models/model0.h5\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2729 - mae: 0.8221 - val_loss: 3.2861 - val_mae: 1.2031\n\nEpoch 00034: val_loss did not improve from 3.28614\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3500 - mae: 0.8340 - val_loss: 3.3118 - val_mae: 1.2053\n\nEpoch 00035: val_loss did not improve from 3.28614\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2708 - mae: 0.8074 - val_loss: 3.2647 - val_mae: 1.1856\n\nEpoch 00036: val_loss improved from 3.28614 to 3.26474, saving model to models/model0.h5\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3775 - mae: 0.8333 - val_loss: 3.2502 - val_mae: 1.1865\n\nEpoch 00037: val_loss improved from 3.26474 to 3.25024, saving model to models/model0.h5\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5272 - mae: 0.8351 - val_loss: 3.2514 - val_mae: 1.1815\n\nEpoch 00038: val_loss did not improve from 3.25024\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2832 - mae: 0.8012 - val_loss: 3.2133 - val_mae: 1.1733\n\nEpoch 00039: val_loss improved from 3.25024 to 3.21329, saving model to models/model0.h5\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2625 - mae: 0.8029 - val_loss: 3.2197 - val_mae: 1.1767\n\nEpoch 00040: val_loss did not improve from 3.21329\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2950 - mae: 0.8082 - val_loss: 3.2097 - val_mae: 1.1707\n\nEpoch 00041: val_loss improved from 3.21329 to 3.20973, saving model to models/model0.h5\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2203 - mae: 0.7857 - val_loss: 3.2023 - val_mae: 1.1691\n\nEpoch 00042: val_loss improved from 3.20973 to 3.20230, saving model to models/model0.h5\nEpoch 43/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.3199 - mae: 0.7966 - val_loss: 3.1825 - val_mae: 1.1709\n\nEpoch 00043: val_loss improved from 3.20230 to 3.18248, saving model to models/model0.h5\nEpoch 44/200\n322/322 [==============================] - 2s 8ms/step - loss: 1.2127 - mae: 0.7741 - val_loss: 3.1741 - val_mae: 1.1652\n\nEpoch 00044: val_loss improved from 3.18248 to 3.17407, saving model to models/model0.h5\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1257 - mae: 0.7676 - val_loss: 3.2026 - val_mae: 1.1680\n\nEpoch 00045: val_loss did not improve from 3.17407\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1363 - mae: 0.7683 - val_loss: 3.2010 - val_mae: 1.1692\n\nEpoch 00046: val_loss did not improve from 3.17407\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3442 - mae: 0.7941 - val_loss: 3.2167 - val_mae: 1.1744\n\nEpoch 00047: val_loss did not improve from 3.17407\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2787 - mae: 0.7831 - val_loss: 3.2698 - val_mae: 1.1834\n\nEpoch 00048: val_loss did not improve from 3.17407\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0747 - mae: 0.7422 - val_loss: 3.2371 - val_mae: 1.1848\n\nEpoch 00049: val_loss did not improve from 3.17407\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1646 - mae: 0.7639 - val_loss: 3.2645 - val_mae: 1.1724\n\nEpoch 00050: val_loss did not improve from 3.17407\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2146 - mae: 0.7676 - val_loss: 3.2746 - val_mae: 1.1692\n\nEpoch 00051: val_loss did not improve from 3.17407\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2468 - mae: 0.7638 - val_loss: 3.2281 - val_mae: 1.1717\n\nEpoch 00052: val_loss did not improve from 3.17407\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1771 - mae: 0.7413 - val_loss: 3.2266 - val_mae: 1.1715\n\nEpoch 00053: val_loss did not improve from 3.17407\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0756 - mae: 0.7379 - val_loss: 3.1760 - val_mae: 1.1680\n\nEpoch 00054: val_loss did not improve from 3.17407\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1131 - mae: 0.7236 - val_loss: 3.2159 - val_mae: 1.1690\n\nEpoch 00055: val_loss did not improve from 3.17407\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0312 - mae: 0.7127 - val_loss: 3.1979 - val_mae: 1.1689\n\nEpoch 00056: val_loss did not improve from 3.17407\nEpoch 57/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0307 - mae: 0.7164 - val_loss: 3.1859 - val_mae: 1.1666\n\nEpoch 00057: val_loss did not improve from 3.17407\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0516 - mae: 0.7206 - val_loss: 3.1657 - val_mae: 1.1600\n\nEpoch 00058: val_loss improved from 3.17407 to 3.16566, saving model to models/model0.h5\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9774 - mae: 0.7100 - val_loss: 3.1745 - val_mae: 1.1633\n\nEpoch 00059: val_loss did not improve from 3.16566\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0710 - mae: 0.7053 - val_loss: 3.1927 - val_mae: 1.1635\n\nEpoch 00060: val_loss did not improve from 3.16566\nEpoch 61/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.0210 - mae: 0.6922 - val_loss: 3.1719 - val_mae: 1.1634\n\nEpoch 00061: val_loss did not improve from 3.16566\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0639 - mae: 0.7198 - val_loss: 3.1626 - val_mae: 1.1546\n\nEpoch 00062: val_loss improved from 3.16566 to 3.16265, saving model to models/model0.h5\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9704 - mae: 0.7002 - val_loss: 3.1653 - val_mae: 1.1535\n\nEpoch 00063: val_loss did not improve from 3.16265\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9557 - mae: 0.7011 - val_loss: 3.1539 - val_mae: 1.1523\n\nEpoch 00064: val_loss improved from 3.16265 to 3.15393, saving model to models/model0.h5\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9151 - mae: 0.6753 - val_loss: 3.1498 - val_mae: 1.1483\n\nEpoch 00065: val_loss improved from 3.15393 to 3.14983, saving model to models/model0.h5\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0183 - mae: 0.6893 - val_loss: 3.1902 - val_mae: 1.1576\n\nEpoch 00066: val_loss did not improve from 3.14983\nEpoch 67/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9874 - mae: 0.7093 - val_loss: 3.1561 - val_mae: 1.1492\n\nEpoch 00067: val_loss did not improve from 3.14983\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9745 - mae: 0.6952 - val_loss: 3.1665 - val_mae: 1.1554\n\nEpoch 00068: val_loss did not improve from 3.14983\nEpoch 69/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0479 - mae: 0.6880 - val_loss: 3.1771 - val_mae: 1.1579\n\nEpoch 00069: val_loss did not improve from 3.14983\nEpoch 70/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9161 - mae: 0.6779 - val_loss: 3.1575 - val_mae: 1.1529\n\nEpoch 00070: val_loss did not improve from 3.14983\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9431 - mae: 0.6864 - val_loss: 3.1705 - val_mae: 1.1615\n\nEpoch 00071: val_loss did not improve from 3.14983\nEpoch 72/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9053 - mae: 0.6763 - val_loss: 3.1810 - val_mae: 1.1554\n\nEpoch 00072: val_loss did not improve from 3.14983\nEpoch 73/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9402 - mae: 0.6743 - val_loss: 3.1632 - val_mae: 1.1570\n\nEpoch 00073: val_loss did not improve from 3.14983\nEpoch 74/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9292 - mae: 0.6967 - val_loss: 3.1683 - val_mae: 1.1530\n\nEpoch 00074: val_loss did not improve from 3.14983\nEpoch 75/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9031 - mae: 0.6643 - val_loss: 3.1903 - val_mae: 1.1559\n\nEpoch 00075: val_loss did not improve from 3.14983\nEpoch 76/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9374 - mae: 0.6803 - val_loss: 3.1834 - val_mae: 1.1535\n\nEpoch 00076: val_loss did not improve from 3.14983\nEpoch 77/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9671 - mae: 0.6728 - val_loss: 3.1904 - val_mae: 1.1565\n\nEpoch 00077: val_loss did not improve from 3.14983\nEpoch 78/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0078 - mae: 0.6886 - val_loss: 3.1915 - val_mae: 1.1574\n\nEpoch 00078: val_loss did not improve from 3.14983\nEpoch 79/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9136 - mae: 0.6762 - val_loss: 3.1846 - val_mae: 1.1562\n\nEpoch 00079: val_loss did not improve from 3.14983\nEpoch 80/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8762 - mae: 0.6664 - val_loss: 3.1872 - val_mae: 1.1567\n\nEpoch 00080: val_loss did not improve from 3.14983\nEpoch 81/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9929 - mae: 0.6659 - val_loss: 3.1880 - val_mae: 1.1556\n\nEpoch 00081: val_loss did not improve from 3.14983\nEpoch 82/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9463 - mae: 0.6808 - val_loss: 3.1861 - val_mae: 1.1545\n\nEpoch 00082: val_loss did not improve from 3.14983\nEpoch 83/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8693 - mae: 0.6627 - val_loss: 3.1884 - val_mae: 1.1558\n\nEpoch 00083: val_loss did not improve from 3.14983\nEpoch 84/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9416 - mae: 0.6691 - val_loss: 3.1904 - val_mae: 1.1566\n\nEpoch 00084: val_loss did not improve from 3.14983\nEpoch 85/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9112 - mae: 0.6699 - val_loss: 3.1858 - val_mae: 1.1572\n\nEpoch 00085: val_loss did not improve from 3.14983\n","output_type":"stream"},{"name":"stderr","text":"Folds:  20%|██        | 1/5 [03:27<13:51, 207.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n322/322 [==============================] - 3s 7ms/step - loss: 4.2253 - mae: 1.4116 - val_loss: 3.5341 - val_mae: 1.3589\n\nEpoch 00001: val_loss improved from inf to 3.53413, saving model to models/model1.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 4.0549 - mae: 1.3730 - val_loss: 3.2076 - val_mae: 1.2281\n\nEpoch 00002: val_loss improved from 3.53413 to 3.20758, saving model to models/model1.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.8023 - mae: 1.3111 - val_loss: 3.0730 - val_mae: 1.2187\n\nEpoch 00003: val_loss improved from 3.20758 to 3.07298, saving model to models/model1.h5\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.5195 - mae: 1.2643 - val_loss: 3.0283 - val_mae: 1.2355\n\nEpoch 00004: val_loss improved from 3.07298 to 3.02834, saving model to models/model1.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4383 - mae: 1.2341 - val_loss: 3.1916 - val_mae: 1.2180\n\nEpoch 00005: val_loss did not improve from 3.02834\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3261 - mae: 1.2405 - val_loss: 3.0048 - val_mae: 1.1933\n\nEpoch 00006: val_loss improved from 3.02834 to 3.00480, saving model to models/model1.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2704 - mae: 1.2178 - val_loss: 3.0635 - val_mae: 1.2336\n\nEpoch 00007: val_loss did not improve from 3.00480\nEpoch 8/200\n322/322 [==============================] - 2s 7ms/step - loss: 3.0459 - mae: 1.1905 - val_loss: 2.8138 - val_mae: 1.1871\n\nEpoch 00008: val_loss improved from 3.00480 to 2.81383, saving model to models/model1.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9076 - mae: 1.1609 - val_loss: 2.7786 - val_mae: 1.1681\n\nEpoch 00009: val_loss improved from 2.81383 to 2.77859, saving model to models/model1.h5\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8433 - mae: 1.1521 - val_loss: 2.7678 - val_mae: 1.1696\n\nEpoch 00010: val_loss improved from 2.77859 to 2.76783, saving model to models/model1.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7187 - mae: 1.1275 - val_loss: 2.7184 - val_mae: 1.1588\n\nEpoch 00011: val_loss improved from 2.76783 to 2.71844, saving model to models/model1.h5\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6800 - mae: 1.1164 - val_loss: 2.8520 - val_mae: 1.1869\n\nEpoch 00012: val_loss did not improve from 2.71844\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6547 - mae: 1.1154 - val_loss: 2.7428 - val_mae: 1.1718\n\nEpoch 00013: val_loss did not improve from 2.71844\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5109 - mae: 1.0868 - val_loss: 2.5909 - val_mae: 1.1485\n\nEpoch 00014: val_loss improved from 2.71844 to 2.59089, saving model to models/model1.h5\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3928 - mae: 1.0668 - val_loss: 2.7160 - val_mae: 1.1512\n\nEpoch 00015: val_loss did not improve from 2.59089\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2860 - mae: 1.0488 - val_loss: 2.7248 - val_mae: 1.1501\n\nEpoch 00016: val_loss did not improve from 2.59089\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1811 - mae: 1.0294 - val_loss: 2.6987 - val_mae: 1.1387\n\nEpoch 00017: val_loss did not improve from 2.59089\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1353 - mae: 1.0258 - val_loss: 2.7760 - val_mae: 1.1653\n\nEpoch 00018: val_loss did not improve from 2.59089\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2857 - mae: 1.0435 - val_loss: 2.5589 - val_mae: 1.1228\n\nEpoch 00019: val_loss improved from 2.59089 to 2.55895, saving model to models/model1.h5\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0236 - mae: 0.9992 - val_loss: 2.7252 - val_mae: 1.1480\n\nEpoch 00020: val_loss did not improve from 2.55895\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8226 - mae: 0.9782 - val_loss: 2.6344 - val_mae: 1.1269\n\nEpoch 00021: val_loss did not improve from 2.55895\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0532 - mae: 0.9802 - val_loss: 2.5419 - val_mae: 1.1375\n\nEpoch 00022: val_loss improved from 2.55895 to 2.54187, saving model to models/model1.h5\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8119 - mae: 0.9680 - val_loss: 2.6451 - val_mae: 1.1607\n\nEpoch 00023: val_loss did not improve from 2.54187\nEpoch 24/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.9580 - mae: 0.9801 - val_loss: 2.5445 - val_mae: 1.1153\n\nEpoch 00024: val_loss did not improve from 2.54187\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8507 - mae: 0.9504 - val_loss: 2.6454 - val_mae: 1.1326\n\nEpoch 00025: val_loss did not improve from 2.54187\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8126 - mae: 0.9648 - val_loss: 2.6886 - val_mae: 1.1458\n\nEpoch 00026: val_loss did not improve from 2.54187\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8354 - mae: 0.9537 - val_loss: 2.5655 - val_mae: 1.1065\n\nEpoch 00027: val_loss did not improve from 2.54187\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8557 - mae: 0.9550 - val_loss: 2.5786 - val_mae: 1.1355\n\nEpoch 00028: val_loss did not improve from 2.54187\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7985 - mae: 0.9417 - val_loss: 2.6919 - val_mae: 1.1801\n\nEpoch 00029: val_loss did not improve from 2.54187\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6963 - mae: 0.9073 - val_loss: 2.5515 - val_mae: 1.1270\n\nEpoch 00030: val_loss did not improve from 2.54187\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5624 - mae: 0.8960 - val_loss: 2.6540 - val_mae: 1.1605\n\nEpoch 00031: val_loss did not improve from 2.54187\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5160 - mae: 0.8810 - val_loss: 2.7367 - val_mae: 1.1639\n\nEpoch 00032: val_loss did not improve from 2.54187\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5925 - mae: 0.8801 - val_loss: 2.5960 - val_mae: 1.1231\n\nEpoch 00033: val_loss did not improve from 2.54187\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2874 - mae: 0.8180 - val_loss: 2.5829 - val_mae: 1.1233\n\nEpoch 00034: val_loss did not improve from 2.54187\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4090 - mae: 0.8384 - val_loss: 2.6219 - val_mae: 1.1367\n\nEpoch 00035: val_loss did not improve from 2.54187\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3167 - mae: 0.8182 - val_loss: 2.5299 - val_mae: 1.1179\n\nEpoch 00036: val_loss improved from 2.54187 to 2.52990, saving model to models/model1.h5\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3319 - mae: 0.8060 - val_loss: 2.5462 - val_mae: 1.1276\n\nEpoch 00037: val_loss did not improve from 2.52990\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3533 - mae: 0.8086 - val_loss: 2.5379 - val_mae: 1.1244\n\nEpoch 00038: val_loss did not improve from 2.52990\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2964 - mae: 0.7990 - val_loss: 2.5299 - val_mae: 1.1162\n\nEpoch 00039: val_loss did not improve from 2.52990\nEpoch 40/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.2222 - mae: 0.7849 - val_loss: 2.4905 - val_mae: 1.1108\n\nEpoch 00040: val_loss improved from 2.52990 to 2.49050, saving model to models/model1.h5\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2638 - mae: 0.7821 - val_loss: 2.4671 - val_mae: 1.1054\n\nEpoch 00041: val_loss improved from 2.49050 to 2.46709, saving model to models/model1.h5\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1381 - mae: 0.7552 - val_loss: 2.5406 - val_mae: 1.1116\n\nEpoch 00042: val_loss did not improve from 2.46709\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1357 - mae: 0.7598 - val_loss: 2.5375 - val_mae: 1.1130\n\nEpoch 00043: val_loss did not improve from 2.46709\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2514 - mae: 0.7825 - val_loss: 2.5009 - val_mae: 1.1021\n\nEpoch 00044: val_loss did not improve from 2.46709\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2249 - mae: 0.7589 - val_loss: 2.4681 - val_mae: 1.0999\n\nEpoch 00045: val_loss did not improve from 2.46709\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2512 - mae: 0.7832 - val_loss: 2.4790 - val_mae: 1.1029\n\nEpoch 00046: val_loss did not improve from 2.46709\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0728 - mae: 0.7371 - val_loss: 2.4566 - val_mae: 1.1019\n\nEpoch 00047: val_loss improved from 2.46709 to 2.45659, saving model to models/model1.h5\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1048 - mae: 0.7563 - val_loss: 2.4987 - val_mae: 1.1046\n\nEpoch 00048: val_loss did not improve from 2.45659\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1145 - mae: 0.7476 - val_loss: 2.4671 - val_mae: 1.0968\n\nEpoch 00049: val_loss did not improve from 2.45659\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1717 - mae: 0.7598 - val_loss: 2.4607 - val_mae: 1.1013\n\nEpoch 00050: val_loss did not improve from 2.45659\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0807 - mae: 0.7383 - val_loss: 2.5091 - val_mae: 1.1062\n\nEpoch 00051: val_loss did not improve from 2.45659\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0667 - mae: 0.7389 - val_loss: 2.4965 - val_mae: 1.1057\n\nEpoch 00052: val_loss did not improve from 2.45659\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1382 - mae: 0.7369 - val_loss: 2.4797 - val_mae: 1.0952\n\nEpoch 00053: val_loss did not improve from 2.45659\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0325 - mae: 0.7241 - val_loss: 2.4960 - val_mae: 1.1065\n\nEpoch 00054: val_loss did not improve from 2.45659\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1454 - mae: 0.7352 - val_loss: 2.4973 - val_mae: 1.1033\n\nEpoch 00055: val_loss did not improve from 2.45659\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9757 - mae: 0.7094 - val_loss: 2.4678 - val_mae: 1.0993\n\nEpoch 00056: val_loss did not improve from 2.45659\nEpoch 57/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.0792 - mae: 0.7337 - val_loss: 2.4896 - val_mae: 1.1076\n\nEpoch 00057: val_loss did not improve from 2.45659\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0698 - mae: 0.7183 - val_loss: 2.4714 - val_mae: 1.0982\n\nEpoch 00058: val_loss did not improve from 2.45659\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0403 - mae: 0.7126 - val_loss: 2.4677 - val_mae: 1.0946\n\nEpoch 00059: val_loss did not improve from 2.45659\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0574 - mae: 0.7137 - val_loss: 2.4452 - val_mae: 1.0892\n\nEpoch 00060: val_loss improved from 2.45659 to 2.44517, saving model to models/model1.h5\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9644 - mae: 0.7032 - val_loss: 2.4672 - val_mae: 1.0951\n\nEpoch 00061: val_loss did not improve from 2.44517\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1402 - mae: 0.7202 - val_loss: 2.4676 - val_mae: 1.0945\n\nEpoch 00062: val_loss did not improve from 2.44517\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0035 - mae: 0.7020 - val_loss: 2.4863 - val_mae: 1.0986\n\nEpoch 00063: val_loss did not improve from 2.44517\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9116 - mae: 0.6806 - val_loss: 2.4580 - val_mae: 1.0896\n\nEpoch 00064: val_loss did not improve from 2.44517\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0075 - mae: 0.6994 - val_loss: 2.4842 - val_mae: 1.0952\n\nEpoch 00065: val_loss did not improve from 2.44517\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8948 - mae: 0.6669 - val_loss: 2.4601 - val_mae: 1.0888\n\nEpoch 00066: val_loss did not improve from 2.44517\nEpoch 67/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9396 - mae: 0.6825 - val_loss: 2.4678 - val_mae: 1.0906\n\nEpoch 00067: val_loss did not improve from 2.44517\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8816 - mae: 0.6723 - val_loss: 2.4731 - val_mae: 1.0933\n\nEpoch 00068: val_loss did not improve from 2.44517\nEpoch 69/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9208 - mae: 0.6784 - val_loss: 2.4563 - val_mae: 1.0950\n\nEpoch 00069: val_loss did not improve from 2.44517\nEpoch 70/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8678 - mae: 0.6637 - val_loss: 2.4561 - val_mae: 1.0879\n\nEpoch 00070: val_loss did not improve from 2.44517\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9690 - mae: 0.6899 - val_loss: 2.4576 - val_mae: 1.0885\n\nEpoch 00071: val_loss did not improve from 2.44517\nEpoch 72/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9384 - mae: 0.6681 - val_loss: 2.4473 - val_mae: 1.0870\n\nEpoch 00072: val_loss did not improve from 2.44517\nEpoch 73/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9215 - mae: 0.6708 - val_loss: 2.4503 - val_mae: 1.0858\n\nEpoch 00073: val_loss did not improve from 2.44517\nEpoch 74/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9175 - mae: 0.6600 - val_loss: 2.4470 - val_mae: 1.0857\n\nEpoch 00074: val_loss did not improve from 2.44517\nEpoch 75/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8784 - mae: 0.6679 - val_loss: 2.4408 - val_mae: 1.0858\n\nEpoch 00075: val_loss improved from 2.44517 to 2.44083, saving model to models/model1.h5\nEpoch 76/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9970 - mae: 0.6630 - val_loss: 2.4439 - val_mae: 1.0859\n\nEpoch 00076: val_loss did not improve from 2.44083\nEpoch 77/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9670 - mae: 0.6735 - val_loss: 2.4363 - val_mae: 1.0858\n\nEpoch 00077: val_loss improved from 2.44083 to 2.43626, saving model to models/model1.h5\nEpoch 78/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9265 - mae: 0.6658 - val_loss: 2.4461 - val_mae: 1.0858\n\nEpoch 00078: val_loss did not improve from 2.43626\nEpoch 79/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8763 - mae: 0.6568 - val_loss: 2.4483 - val_mae: 1.0881\n\nEpoch 00079: val_loss did not improve from 2.43626\nEpoch 80/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8592 - mae: 0.6666 - val_loss: 2.4500 - val_mae: 1.0879\n\nEpoch 00080: val_loss did not improve from 2.43626\nEpoch 81/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9749 - mae: 0.6767 - val_loss: 2.4556 - val_mae: 1.0899\n\nEpoch 00081: val_loss did not improve from 2.43626\nEpoch 82/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8960 - mae: 0.6653 - val_loss: 2.4419 - val_mae: 1.0867\n\nEpoch 00082: val_loss did not improve from 2.43626\nEpoch 83/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8790 - mae: 0.6567 - val_loss: 2.4518 - val_mae: 1.0873\n\nEpoch 00083: val_loss did not improve from 2.43626\nEpoch 84/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9044 - mae: 0.6720 - val_loss: 2.4584 - val_mae: 1.0912\n\nEpoch 00084: val_loss did not improve from 2.43626\nEpoch 85/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9030 - mae: 0.6671 - val_loss: 2.4623 - val_mae: 1.0932\n\nEpoch 00085: val_loss did not improve from 2.43626\nEpoch 86/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8786 - mae: 0.6549 - val_loss: 2.4470 - val_mae: 1.0898\n\nEpoch 00086: val_loss did not improve from 2.43626\nEpoch 87/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8665 - mae: 0.6435 - val_loss: 2.4470 - val_mae: 1.0879\n\nEpoch 00087: val_loss did not improve from 2.43626\nEpoch 88/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8716 - mae: 0.6621 - val_loss: 2.4527 - val_mae: 1.0887\n\nEpoch 00088: val_loss did not improve from 2.43626\nEpoch 89/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.9234 - mae: 0.6700 - val_loss: 2.4489 - val_mae: 1.0888\n\nEpoch 00089: val_loss did not improve from 2.43626\nEpoch 90/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9296 - mae: 0.6608 - val_loss: 2.4487 - val_mae: 1.0892\n\nEpoch 00090: val_loss did not improve from 2.43626\nEpoch 91/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8694 - mae: 0.6608 - val_loss: 2.4505 - val_mae: 1.0889\n\nEpoch 00091: val_loss did not improve from 2.43626\nEpoch 92/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9159 - mae: 0.6643 - val_loss: 2.4541 - val_mae: 1.0883\n\nEpoch 00092: val_loss did not improve from 2.43626\nEpoch 93/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8734 - mae: 0.6536 - val_loss: 2.4523 - val_mae: 1.0885\n\nEpoch 00093: val_loss did not improve from 2.43626\nEpoch 94/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9151 - mae: 0.6582 - val_loss: 2.4474 - val_mae: 1.0885\n\nEpoch 00094: val_loss did not improve from 2.43626\nEpoch 95/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0022 - mae: 0.6772 - val_loss: 2.4455 - val_mae: 1.0879\n\nEpoch 00095: val_loss did not improve from 2.43626\nEpoch 96/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8363 - mae: 0.6487 - val_loss: 2.4464 - val_mae: 1.0874\n\nEpoch 00096: val_loss did not improve from 2.43626\nEpoch 97/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0342 - mae: 0.6742 - val_loss: 2.4453 - val_mae: 1.0873\n","output_type":"stream"},{"name":"stderr","text":"Folds:  40%|████      | 2/5 [06:38<09:53, 197.83s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00097: val_loss did not improve from 2.43626\nEpoch 1/200\n322/322 [==============================] - 3s 7ms/step - loss: 4.0724 - mae: 1.4133 - val_loss: 4.3173 - val_mae: 1.3726\n\nEpoch 00001: val_loss improved from inf to 4.31726, saving model to models/model2.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.8364 - mae: 1.3549 - val_loss: 4.0766 - val_mae: 1.3140\n\nEpoch 00002: val_loss improved from 4.31726 to 4.07658, saving model to models/model2.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.6263 - mae: 1.3116 - val_loss: 4.2725 - val_mae: 1.3493\n\nEpoch 00003: val_loss did not improve from 4.07658\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.4493 - mae: 1.2630 - val_loss: 3.7082 - val_mae: 1.2470\n\nEpoch 00004: val_loss improved from 4.07658 to 3.70823, saving model to models/model2.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3440 - mae: 1.2497 - val_loss: 3.6282 - val_mae: 1.2250\n\nEpoch 00005: val_loss improved from 3.70823 to 3.62824, saving model to models/model2.h5\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1589 - mae: 1.2108 - val_loss: 3.6339 - val_mae: 1.2198\n\nEpoch 00006: val_loss did not improve from 3.62824\nEpoch 7/200\n322/322 [==============================] - 2s 7ms/step - loss: 3.0707 - mae: 1.1973 - val_loss: 3.5146 - val_mae: 1.2053\n\nEpoch 00007: val_loss improved from 3.62824 to 3.51463, saving model to models/model2.h5\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9648 - mae: 1.1776 - val_loss: 3.4725 - val_mae: 1.2163\n\nEpoch 00008: val_loss improved from 3.51463 to 3.47248, saving model to models/model2.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.8417 - mae: 1.1432 - val_loss: 3.3609 - val_mae: 1.1814\n\nEpoch 00009: val_loss improved from 3.47248 to 3.36091, saving model to models/model2.h5\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7616 - mae: 1.1310 - val_loss: 3.3465 - val_mae: 1.1946\n\nEpoch 00010: val_loss improved from 3.36091 to 3.34651, saving model to models/model2.h5\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6761 - mae: 1.1265 - val_loss: 3.4053 - val_mae: 1.1888\n\nEpoch 00011: val_loss did not improve from 3.34651\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4836 - mae: 1.0909 - val_loss: 3.3379 - val_mae: 1.1819\n\nEpoch 00012: val_loss improved from 3.34651 to 3.33788, saving model to models/model2.h5\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4295 - mae: 1.0887 - val_loss: 3.4876 - val_mae: 1.2281\n\nEpoch 00013: val_loss did not improve from 3.33788\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3904 - mae: 1.0802 - val_loss: 3.3713 - val_mae: 1.2268\n\nEpoch 00014: val_loss did not improve from 3.33788\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2227 - mae: 1.0390 - val_loss: 3.2925 - val_mae: 1.2061\n\nEpoch 00015: val_loss improved from 3.33788 to 3.29251, saving model to models/model2.h5\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1582 - mae: 1.0398 - val_loss: 3.2528 - val_mae: 1.2159\n\nEpoch 00016: val_loss improved from 3.29251 to 3.25281, saving model to models/model2.h5\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1615 - mae: 1.0263 - val_loss: 3.2725 - val_mae: 1.1876\n\nEpoch 00017: val_loss did not improve from 3.25281\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1928 - mae: 1.0188 - val_loss: 3.0866 - val_mae: 1.1550\n\nEpoch 00018: val_loss improved from 3.25281 to 3.08664, saving model to models/model2.h5\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9514 - mae: 0.9724 - val_loss: 3.1314 - val_mae: 1.1754\n\nEpoch 00019: val_loss did not improve from 3.08664\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8089 - mae: 0.9463 - val_loss: 3.0008 - val_mae: 1.1526\n\nEpoch 00020: val_loss improved from 3.08664 to 3.00084, saving model to models/model2.h5\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8676 - mae: 0.9595 - val_loss: 3.0378 - val_mae: 1.1622\n\nEpoch 00021: val_loss did not improve from 3.00084\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7813 - mae: 0.9403 - val_loss: 3.0310 - val_mae: 1.1520\n\nEpoch 00022: val_loss did not improve from 3.00084\nEpoch 23/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8021 - mae: 0.9343 - val_loss: 3.0458 - val_mae: 1.1344\n\nEpoch 00023: val_loss did not improve from 3.00084\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8088 - mae: 0.9238 - val_loss: 3.1091 - val_mae: 1.1650\n\nEpoch 00024: val_loss did not improve from 3.00084\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8044 - mae: 0.9428 - val_loss: 3.1008 - val_mae: 1.1711\n\nEpoch 00025: val_loss did not improve from 3.00084\nEpoch 26/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.7101 - mae: 0.9244 - val_loss: 3.0873 - val_mae: 1.1606\n\nEpoch 00026: val_loss did not improve from 3.00084\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6405 - mae: 0.9100 - val_loss: 3.3238 - val_mae: 1.2007\n\nEpoch 00027: val_loss did not improve from 3.00084\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6814 - mae: 0.9089 - val_loss: 3.0650 - val_mae: 1.1616\n\nEpoch 00028: val_loss did not improve from 3.00084\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5528 - mae: 0.8733 - val_loss: 2.9884 - val_mae: 1.1615\n\nEpoch 00029: val_loss improved from 3.00084 to 2.98835, saving model to models/model2.h5\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5614 - mae: 0.8872 - val_loss: 3.1607 - val_mae: 1.1724\n\nEpoch 00030: val_loss did not improve from 2.98835\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5362 - mae: 0.8848 - val_loss: 3.0131 - val_mae: 1.1524\n\nEpoch 00031: val_loss did not improve from 2.98835\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6383 - mae: 0.9064 - val_loss: 3.1827 - val_mae: 1.1679\n\nEpoch 00032: val_loss did not improve from 2.98835\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3949 - mae: 0.8336 - val_loss: 3.0328 - val_mae: 1.1580\n\nEpoch 00033: val_loss did not improve from 2.98835\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5849 - mae: 0.8817 - val_loss: 2.9626 - val_mae: 1.1572\n\nEpoch 00034: val_loss improved from 2.98835 to 2.96255, saving model to models/model2.h5\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5375 - mae: 0.8781 - val_loss: 3.2112 - val_mae: 1.1723\n\nEpoch 00035: val_loss did not improve from 2.96255\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3928 - mae: 0.8484 - val_loss: 3.0103 - val_mae: 1.1573\n\nEpoch 00036: val_loss did not improve from 2.96255\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5047 - mae: 0.8546 - val_loss: 2.9364 - val_mae: 1.1527\n\nEpoch 00037: val_loss improved from 2.96255 to 2.93642, saving model to models/model2.h5\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4589 - mae: 0.8528 - val_loss: 2.9292 - val_mae: 1.1509\n\nEpoch 00038: val_loss improved from 2.93642 to 2.92923, saving model to models/model2.h5\nEpoch 39/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.4392 - mae: 0.8458 - val_loss: 3.0075 - val_mae: 1.1629\n\nEpoch 00039: val_loss did not improve from 2.92923\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3640 - mae: 0.8295 - val_loss: 2.9448 - val_mae: 1.1635\n\nEpoch 00040: val_loss did not improve from 2.92923\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3650 - mae: 0.8414 - val_loss: 3.1019 - val_mae: 1.1621\n\nEpoch 00041: val_loss did not improve from 2.92923\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3986 - mae: 0.8455 - val_loss: 3.1155 - val_mae: 1.1741\n\nEpoch 00042: val_loss did not improve from 2.92923\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3979 - mae: 0.8471 - val_loss: 3.1012 - val_mae: 1.1657\n\nEpoch 00043: val_loss did not improve from 2.92923\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3384 - mae: 0.8277 - val_loss: 3.0955 - val_mae: 1.1784\n\nEpoch 00044: val_loss did not improve from 2.92923\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3367 - mae: 0.8227 - val_loss: 2.9900 - val_mae: 1.1575\n\nEpoch 00045: val_loss did not improve from 2.92923\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3554 - mae: 0.8311 - val_loss: 2.8435 - val_mae: 1.1205\n\nEpoch 00046: val_loss improved from 2.92923 to 2.84355, saving model to models/model2.h5\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3179 - mae: 0.8278 - val_loss: 2.9972 - val_mae: 1.1615\n\nEpoch 00047: val_loss did not improve from 2.84355\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3816 - mae: 0.8358 - val_loss: 2.9627 - val_mae: 1.1576\n\nEpoch 00048: val_loss did not improve from 2.84355\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1714 - mae: 0.7875 - val_loss: 2.9368 - val_mae: 1.1527\n\nEpoch 00049: val_loss did not improve from 2.84355\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3042 - mae: 0.8181 - val_loss: 3.0098 - val_mae: 1.1482\n\nEpoch 00050: val_loss did not improve from 2.84355\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2826 - mae: 0.7996 - val_loss: 3.0667 - val_mae: 1.1521\n\nEpoch 00051: val_loss did not improve from 2.84355\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2017 - mae: 0.7901 - val_loss: 2.9937 - val_mae: 1.1602\n\nEpoch 00052: val_loss did not improve from 2.84355\nEpoch 53/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3351 - mae: 0.8101 - val_loss: 3.2598 - val_mae: 1.2013\n\nEpoch 00053: val_loss did not improve from 2.84355\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1938 - mae: 0.7902 - val_loss: 3.0011 - val_mae: 1.1470\n\nEpoch 00054: val_loss did not improve from 2.84355\nEpoch 55/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.1702 - mae: 0.7767 - val_loss: 2.9769 - val_mae: 1.1466\n\nEpoch 00055: val_loss did not improve from 2.84355\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2614 - mae: 0.8076 - val_loss: 2.9339 - val_mae: 1.1659\n\nEpoch 00056: val_loss did not improve from 2.84355\nEpoch 57/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0846 - mae: 0.7506 - val_loss: 2.9739 - val_mae: 1.1601\n\nEpoch 00057: val_loss did not improve from 2.84355\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0737 - mae: 0.7454 - val_loss: 2.9247 - val_mae: 1.1468\n\nEpoch 00058: val_loss did not improve from 2.84355\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0392 - mae: 0.7105 - val_loss: 2.9626 - val_mae: 1.1574\n\nEpoch 00059: val_loss did not improve from 2.84355\nEpoch 60/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.0356 - mae: 0.7202 - val_loss: 2.9835 - val_mae: 1.1531\n\nEpoch 00060: val_loss did not improve from 2.84355\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0710 - mae: 0.7173 - val_loss: 2.9518 - val_mae: 1.1437\n\nEpoch 00061: val_loss did not improve from 2.84355\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0814 - mae: 0.7103 - val_loss: 2.9726 - val_mae: 1.1357\n\nEpoch 00062: val_loss did not improve from 2.84355\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0129 - mae: 0.7099 - val_loss: 2.9989 - val_mae: 1.1443\n\nEpoch 00063: val_loss did not improve from 2.84355\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9186 - mae: 0.6798 - val_loss: 2.9483 - val_mae: 1.1355\n\nEpoch 00064: val_loss did not improve from 2.84355\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9202 - mae: 0.6818 - val_loss: 2.9291 - val_mae: 1.1415\n\nEpoch 00065: val_loss did not improve from 2.84355\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0279 - mae: 0.7100 - val_loss: 2.9738 - val_mae: 1.1430\n","output_type":"stream"},{"name":"stderr","text":"Folds:  60%|██████    | 3/5 [08:50<05:35, 167.61s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00066: val_loss did not improve from 2.84355\nEpoch 1/200\n322/322 [==============================] - 3s 7ms/step - loss: 4.1990 - mae: 1.4243 - val_loss: 4.1620 - val_mae: 1.3367\n\nEpoch 00001: val_loss improved from inf to 4.16198, saving model to models/model3.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.9435 - mae: 1.3596 - val_loss: 3.8476 - val_mae: 1.3276\n\nEpoch 00002: val_loss improved from 4.16198 to 3.84763, saving model to models/model3.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.6428 - mae: 1.2915 - val_loss: 3.4137 - val_mae: 1.1863\n\nEpoch 00003: val_loss improved from 3.84763 to 3.41374, saving model to models/model3.h5\nEpoch 4/200\n322/322 [==============================] - 2s 8ms/step - loss: 3.4082 - mae: 1.2575 - val_loss: 3.3939 - val_mae: 1.1742\n\nEpoch 00004: val_loss improved from 3.41374 to 3.39392, saving model to models/model3.h5\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.3373 - mae: 1.2459 - val_loss: 3.9774 - val_mae: 1.3035\n\nEpoch 00005: val_loss did not improve from 3.39392\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2067 - mae: 1.2169 - val_loss: 3.3809 - val_mae: 1.1749\n\nEpoch 00006: val_loss improved from 3.39392 to 3.38095, saving model to models/model3.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.1728 - mae: 1.2100 - val_loss: 3.5385 - val_mae: 1.2028\n\nEpoch 00007: val_loss did not improve from 3.38095\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.0259 - mae: 1.1965 - val_loss: 3.5077 - val_mae: 1.2183\n\nEpoch 00008: val_loss did not improve from 3.38095\nEpoch 9/200\n322/322 [==============================] - 2s 7ms/step - loss: 2.7750 - mae: 1.1416 - val_loss: 3.2541 - val_mae: 1.1640\n\nEpoch 00009: val_loss improved from 3.38095 to 3.25407, saving model to models/model3.h5\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.6543 - mae: 1.1243 - val_loss: 3.6434 - val_mae: 1.2310\n\nEpoch 00010: val_loss did not improve from 3.25407\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4986 - mae: 1.0825 - val_loss: 3.3516 - val_mae: 1.1837\n\nEpoch 00011: val_loss did not improve from 3.25407\nEpoch 12/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4608 - mae: 1.0710 - val_loss: 3.2214 - val_mae: 1.1339\n\nEpoch 00012: val_loss improved from 3.25407 to 3.22139, saving model to models/model3.h5\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3827 - mae: 1.0613 - val_loss: 3.2762 - val_mae: 1.1551\n\nEpoch 00013: val_loss did not improve from 3.22139\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2812 - mae: 1.0547 - val_loss: 3.3205 - val_mae: 1.1937\n\nEpoch 00014: val_loss did not improve from 3.22139\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0357 - mae: 1.0052 - val_loss: 3.2391 - val_mae: 1.1745\n\nEpoch 00015: val_loss did not improve from 3.22139\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.0264 - mae: 0.9950 - val_loss: 3.4463 - val_mae: 1.2127\n\nEpoch 00016: val_loss did not improve from 3.22139\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9790 - mae: 0.9828 - val_loss: 3.1887 - val_mae: 1.1717\n\nEpoch 00017: val_loss improved from 3.22139 to 3.18874, saving model to models/model3.h5\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7947 - mae: 0.9626 - val_loss: 2.9621 - val_mae: 1.1095\n\nEpoch 00018: val_loss improved from 3.18874 to 2.96209, saving model to models/model3.h5\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7568 - mae: 0.9492 - val_loss: 2.9553 - val_mae: 1.1356\n\nEpoch 00019: val_loss improved from 2.96209 to 2.95529, saving model to models/model3.h5\nEpoch 20/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.6841 - mae: 0.9354 - val_loss: 3.0412 - val_mae: 1.1596\n\nEpoch 00020: val_loss did not improve from 2.95529\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8509 - mae: 0.9578 - val_loss: 2.9590 - val_mae: 1.1312\n\nEpoch 00021: val_loss did not improve from 2.95529\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7154 - mae: 0.9220 - val_loss: 3.1231 - val_mae: 1.1399\n\nEpoch 00022: val_loss did not improve from 2.95529\nEpoch 23/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9653 - mae: 0.9811 - val_loss: 3.1935 - val_mae: 1.1620\n\nEpoch 00023: val_loss did not improve from 2.95529\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6100 - mae: 0.9055 - val_loss: 3.1843 - val_mae: 1.1625\n\nEpoch 00024: val_loss did not improve from 2.95529\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5770 - mae: 0.8973 - val_loss: 2.9153 - val_mae: 1.1156\n\nEpoch 00025: val_loss improved from 2.95529 to 2.91527, saving model to models/model3.h5\nEpoch 26/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.4924 - mae: 0.8823 - val_loss: 2.9000 - val_mae: 1.1089\n\nEpoch 00026: val_loss improved from 2.91527 to 2.90002, saving model to models/model3.h5\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3968 - mae: 0.8628 - val_loss: 2.8913 - val_mae: 1.1135\n\nEpoch 00027: val_loss improved from 2.90002 to 2.89128, saving model to models/model3.h5\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5071 - mae: 0.8664 - val_loss: 3.0644 - val_mae: 1.1210\n\nEpoch 00028: val_loss did not improve from 2.89128\nEpoch 29/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5535 - mae: 0.8983 - val_loss: 3.0961 - val_mae: 1.1372\n\nEpoch 00029: val_loss did not improve from 2.89128\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3876 - mae: 0.8434 - val_loss: 2.9720 - val_mae: 1.1192\n\nEpoch 00030: val_loss did not improve from 2.89128\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4464 - mae: 0.8632 - val_loss: 3.0400 - val_mae: 1.1259\n\nEpoch 00031: val_loss did not improve from 2.89128\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4362 - mae: 0.8523 - val_loss: 3.0143 - val_mae: 1.1220\n\nEpoch 00032: val_loss did not improve from 2.89128\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3223 - mae: 0.8314 - val_loss: 3.0483 - val_mae: 1.1331\n\nEpoch 00033: val_loss did not improve from 2.89128\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3406 - mae: 0.8343 - val_loss: 2.9819 - val_mae: 1.1030\n\nEpoch 00034: val_loss did not improve from 2.89128\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2945 - mae: 0.8304 - val_loss: 2.8766 - val_mae: 1.0997\n\nEpoch 00035: val_loss improved from 2.89128 to 2.87656, saving model to models/model3.h5\nEpoch 36/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.5691 - mae: 0.8628 - val_loss: 3.1486 - val_mae: 1.1535\n\nEpoch 00036: val_loss did not improve from 2.87656\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3296 - mae: 0.8315 - val_loss: 3.0003 - val_mae: 1.1190\n\nEpoch 00037: val_loss did not improve from 2.87656\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2766 - mae: 0.8209 - val_loss: 3.1195 - val_mae: 1.1325\n\nEpoch 00038: val_loss did not improve from 2.87656\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2881 - mae: 0.8124 - val_loss: 2.9578 - val_mae: 1.1188\n\nEpoch 00039: val_loss did not improve from 2.87656\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2248 - mae: 0.7992 - val_loss: 2.9656 - val_mae: 1.1201\n\nEpoch 00040: val_loss did not improve from 2.87656\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2137 - mae: 0.7893 - val_loss: 2.9547 - val_mae: 1.1137\n\nEpoch 00041: val_loss did not improve from 2.87656\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1401 - mae: 0.7795 - val_loss: 2.8241 - val_mae: 1.1033\n\nEpoch 00042: val_loss improved from 2.87656 to 2.82413, saving model to models/model3.h5\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3684 - mae: 0.8228 - val_loss: 3.0015 - val_mae: 1.1235\n\nEpoch 00043: val_loss did not improve from 2.82413\nEpoch 44/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4059 - mae: 0.8158 - val_loss: 2.8609 - val_mae: 1.1212\n\nEpoch 00044: val_loss did not improve from 2.82413\nEpoch 45/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2220 - mae: 0.7894 - val_loss: 2.8893 - val_mae: 1.1103\n\nEpoch 00045: val_loss did not improve from 2.82413\nEpoch 46/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1763 - mae: 0.7908 - val_loss: 2.9377 - val_mae: 1.1090\n\nEpoch 00046: val_loss did not improve from 2.82413\nEpoch 47/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1684 - mae: 0.7786 - val_loss: 2.8149 - val_mae: 1.0904\n\nEpoch 00047: val_loss improved from 2.82413 to 2.81490, saving model to models/model3.h5\nEpoch 48/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2624 - mae: 0.7862 - val_loss: 2.8602 - val_mae: 1.0997\n\nEpoch 00048: val_loss did not improve from 2.81490\nEpoch 49/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1477 - mae: 0.7682 - val_loss: 2.8908 - val_mae: 1.1120\n\nEpoch 00049: val_loss did not improve from 2.81490\nEpoch 50/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3124 - mae: 0.7771 - val_loss: 3.2416 - val_mae: 1.1891\n\nEpoch 00050: val_loss did not improve from 2.81490\nEpoch 51/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.4807 - mae: 0.8576 - val_loss: 3.0535 - val_mae: 1.1374\n\nEpoch 00051: val_loss did not improve from 2.81490\nEpoch 52/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1177 - mae: 0.7566 - val_loss: 2.8688 - val_mae: 1.1009\n\nEpoch 00052: val_loss did not improve from 2.81490\nEpoch 53/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.0304 - mae: 0.7287 - val_loss: 2.9260 - val_mae: 1.1059\n\nEpoch 00053: val_loss did not improve from 2.81490\nEpoch 54/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0322 - mae: 0.7330 - val_loss: 2.9205 - val_mae: 1.1132\n\nEpoch 00054: val_loss did not improve from 2.81490\nEpoch 55/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0619 - mae: 0.7340 - val_loss: 2.9038 - val_mae: 1.1074\n\nEpoch 00055: val_loss did not improve from 2.81490\nEpoch 56/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0691 - mae: 0.7443 - val_loss: 2.7554 - val_mae: 1.0737\n\nEpoch 00056: val_loss improved from 2.81490 to 2.75539, saving model to models/model3.h5\nEpoch 57/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0998 - mae: 0.7387 - val_loss: 2.8287 - val_mae: 1.1053\n\nEpoch 00057: val_loss did not improve from 2.75539\nEpoch 58/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1180 - mae: 0.7513 - val_loss: 2.8966 - val_mae: 1.1066\n\nEpoch 00058: val_loss did not improve from 2.75539\nEpoch 59/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0862 - mae: 0.7350 - val_loss: 2.8813 - val_mae: 1.1016\n\nEpoch 00059: val_loss did not improve from 2.75539\nEpoch 60/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1069 - mae: 0.7524 - val_loss: 2.9067 - val_mae: 1.1161\n\nEpoch 00060: val_loss did not improve from 2.75539\nEpoch 61/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1759 - mae: 0.7787 - val_loss: 2.8427 - val_mae: 1.1124\n\nEpoch 00061: val_loss did not improve from 2.75539\nEpoch 62/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2239 - mae: 0.7758 - val_loss: 2.8474 - val_mae: 1.1185\n\nEpoch 00062: val_loss did not improve from 2.75539\nEpoch 63/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1408 - mae: 0.7669 - val_loss: 2.8853 - val_mae: 1.1026\n\nEpoch 00063: val_loss did not improve from 2.75539\nEpoch 64/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0606 - mae: 0.7483 - val_loss: 2.8198 - val_mae: 1.0933\n\nEpoch 00064: val_loss did not improve from 2.75539\nEpoch 65/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.9945 - mae: 0.7251 - val_loss: 2.8651 - val_mae: 1.1096\n\nEpoch 00065: val_loss did not improve from 2.75539\nEpoch 66/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0001 - mae: 0.7162 - val_loss: 2.8661 - val_mae: 1.1082\n\nEpoch 00066: val_loss did not improve from 2.75539\nEpoch 67/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0138 - mae: 0.7150 - val_loss: 2.7817 - val_mae: 1.0859\n\nEpoch 00067: val_loss did not improve from 2.75539\nEpoch 68/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8237 - mae: 0.6476 - val_loss: 2.7689 - val_mae: 1.0805\n\nEpoch 00068: val_loss did not improve from 2.75539\nEpoch 69/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.8105 - mae: 0.6545 - val_loss: 2.8673 - val_mae: 1.1154\n\nEpoch 00069: val_loss did not improve from 2.75539\nEpoch 70/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8461 - mae: 0.6456 - val_loss: 2.7806 - val_mae: 1.0782\n\nEpoch 00070: val_loss did not improve from 2.75539\nEpoch 71/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7866 - mae: 0.6463 - val_loss: 2.7965 - val_mae: 1.0863\n\nEpoch 00071: val_loss did not improve from 2.75539\nEpoch 72/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7995 - mae: 0.6316 - val_loss: 2.8054 - val_mae: 1.0768\n\nEpoch 00072: val_loss did not improve from 2.75539\nEpoch 73/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7852 - mae: 0.6189 - val_loss: 2.8042 - val_mae: 1.0793\n\nEpoch 00073: val_loss did not improve from 2.75539\nEpoch 74/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7922 - mae: 0.6450 - val_loss: 2.7796 - val_mae: 1.0762\n\nEpoch 00074: val_loss did not improve from 2.75539\nEpoch 75/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8298 - mae: 0.6292 - val_loss: 2.7849 - val_mae: 1.0750\n\nEpoch 00075: val_loss did not improve from 2.75539\nEpoch 76/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7493 - mae: 0.6196 - val_loss: 2.7431 - val_mae: 1.0703\n\nEpoch 00076: val_loss improved from 2.75539 to 2.74312, saving model to models/model3.h5\nEpoch 77/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.8318 - mae: 0.6313 - val_loss: 2.7618 - val_mae: 1.0801\n\nEpoch 00077: val_loss did not improve from 2.74312\nEpoch 78/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7399 - mae: 0.6106 - val_loss: 2.7433 - val_mae: 1.0824\n\nEpoch 00078: val_loss did not improve from 2.74312\nEpoch 79/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7856 - mae: 0.6283 - val_loss: 2.8138 - val_mae: 1.0923\n\nEpoch 00079: val_loss did not improve from 2.74312\nEpoch 80/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7109 - mae: 0.5965 - val_loss: 2.7628 - val_mae: 1.0776\n\nEpoch 00080: val_loss did not improve from 2.74312\nEpoch 81/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7046 - mae: 0.6060 - val_loss: 2.7621 - val_mae: 1.0892\n\nEpoch 00081: val_loss did not improve from 2.74312\nEpoch 82/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7859 - mae: 0.6064 - val_loss: 2.7640 - val_mae: 1.0830\n\nEpoch 00082: val_loss did not improve from 2.74312\nEpoch 83/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7219 - mae: 0.6053 - val_loss: 2.7790 - val_mae: 1.0899\n\nEpoch 00083: val_loss did not improve from 2.74312\nEpoch 84/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7350 - mae: 0.6015 - val_loss: 2.7495 - val_mae: 1.0913\n\nEpoch 00084: val_loss did not improve from 2.74312\nEpoch 85/200\n322/322 [==============================] - 2s 7ms/step - loss: 0.8369 - mae: 0.6248 - val_loss: 2.7469 - val_mae: 1.0890\n\nEpoch 00085: val_loss did not improve from 2.74312\nEpoch 86/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7137 - mae: 0.6013 - val_loss: 2.7809 - val_mae: 1.0987\n\nEpoch 00086: val_loss did not improve from 2.74312\nEpoch 87/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7157 - mae: 0.6007 - val_loss: 2.7619 - val_mae: 1.0935\n\nEpoch 00087: val_loss did not improve from 2.74312\nEpoch 88/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7607 - mae: 0.5896 - val_loss: 2.7666 - val_mae: 1.0926\n\nEpoch 00088: val_loss did not improve from 2.74312\nEpoch 89/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6989 - mae: 0.5984 - val_loss: 2.7458 - val_mae: 1.0914\n\nEpoch 00089: val_loss did not improve from 2.74312\nEpoch 90/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7057 - mae: 0.5833 - val_loss: 2.7486 - val_mae: 1.0931\n\nEpoch 00090: val_loss did not improve from 2.74312\nEpoch 91/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.7156 - mae: 0.5763 - val_loss: 2.7634 - val_mae: 1.0936\n\nEpoch 00091: val_loss did not improve from 2.74312\nEpoch 92/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6693 - mae: 0.5812 - val_loss: 2.7827 - val_mae: 1.0905\n\nEpoch 00092: val_loss did not improve from 2.74312\nEpoch 93/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6632 - mae: 0.5832 - val_loss: 2.7584 - val_mae: 1.0868\n\nEpoch 00093: val_loss did not improve from 2.74312\nEpoch 94/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6676 - mae: 0.5816 - val_loss: 2.7697 - val_mae: 1.0917\n\nEpoch 00094: val_loss did not improve from 2.74312\nEpoch 95/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6594 - mae: 0.5757 - val_loss: 2.7654 - val_mae: 1.0910\n\nEpoch 00095: val_loss did not improve from 2.74312\nEpoch 96/200\n322/322 [==============================] - 2s 6ms/step - loss: 0.6444 - mae: 0.5608 - val_loss: 2.7538 - val_mae: 1.0858\n\nEpoch 00096: val_loss did not improve from 2.74312\n","output_type":"stream"},{"name":"stderr","text":"Folds:  80%|████████  | 4/5 [12:14<03:02, 182.15s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n322/322 [==============================] - 3s 8ms/step - loss: 4.1801 - mae: 1.4084 - val_loss: 3.2907 - val_mae: 1.3094\n\nEpoch 00001: val_loss improved from inf to 3.29074, saving model to models/model4.h5\nEpoch 2/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.9285 - mae: 1.3414 - val_loss: 2.9169 - val_mae: 1.2033\n\nEpoch 00002: val_loss improved from 3.29074 to 2.91694, saving model to models/model4.h5\nEpoch 3/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.7837 - mae: 1.3115 - val_loss: 2.7630 - val_mae: 1.1579\n\nEpoch 00003: val_loss improved from 2.91694 to 2.76297, saving model to models/model4.h5\nEpoch 4/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.5778 - mae: 1.2722 - val_loss: 2.8443 - val_mae: 1.1631\n\nEpoch 00004: val_loss did not improve from 2.76297\nEpoch 5/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.5367 - mae: 1.2656 - val_loss: 3.3709 - val_mae: 1.2948\n\nEpoch 00005: val_loss did not improve from 2.76297\nEpoch 6/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2880 - mae: 1.2148 - val_loss: 2.6432 - val_mae: 1.1342\n\nEpoch 00006: val_loss improved from 2.76297 to 2.64318, saving model to models/model4.h5\nEpoch 7/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2843 - mae: 1.2149 - val_loss: 2.6054 - val_mae: 1.1186\n\nEpoch 00007: val_loss improved from 2.64318 to 2.60544, saving model to models/model4.h5\nEpoch 8/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.2452 - mae: 1.2186 - val_loss: 2.5098 - val_mae: 1.1200\n\nEpoch 00008: val_loss improved from 2.60544 to 2.50982, saving model to models/model4.h5\nEpoch 9/200\n322/322 [==============================] - 2s 6ms/step - loss: 3.0074 - mae: 1.1715 - val_loss: 2.8819 - val_mae: 1.2177\n\nEpoch 00009: val_loss did not improve from 2.50982\nEpoch 10/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.9559 - mae: 1.1692 - val_loss: 2.8014 - val_mae: 1.1986\n\nEpoch 00010: val_loss did not improve from 2.50982\nEpoch 11/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.7458 - mae: 1.1264 - val_loss: 2.6510 - val_mae: 1.1573\n\nEpoch 00011: val_loss did not improve from 2.50982\nEpoch 12/200\n322/322 [==============================] - 2s 8ms/step - loss: 2.6052 - mae: 1.1114 - val_loss: 2.5874 - val_mae: 1.1335\n\nEpoch 00012: val_loss did not improve from 2.50982\nEpoch 13/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.5331 - mae: 1.0974 - val_loss: 2.7226 - val_mae: 1.1538\n\nEpoch 00013: val_loss did not improve from 2.50982\nEpoch 14/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.3243 - mae: 1.0533 - val_loss: 2.7556 - val_mae: 1.1685\n\nEpoch 00014: val_loss did not improve from 2.50982\nEpoch 15/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.4580 - mae: 1.0792 - val_loss: 2.5929 - val_mae: 1.1391\n\nEpoch 00015: val_loss did not improve from 2.50982\nEpoch 16/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.2062 - mae: 1.0305 - val_loss: 2.6467 - val_mae: 1.1507\n\nEpoch 00016: val_loss did not improve from 2.50982\nEpoch 17/200\n322/322 [==============================] - 2s 6ms/step - loss: 2.1965 - mae: 1.0274 - val_loss: 2.4568 - val_mae: 1.1072\n\nEpoch 00017: val_loss improved from 2.50982 to 2.45682, saving model to models/model4.h5\nEpoch 18/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9557 - mae: 0.9807 - val_loss: 2.5148 - val_mae: 1.1375\n\nEpoch 00018: val_loss did not improve from 2.45682\nEpoch 19/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8775 - mae: 0.9706 - val_loss: 2.5974 - val_mae: 1.1491\n\nEpoch 00019: val_loss did not improve from 2.45682\nEpoch 20/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9763 - mae: 0.9895 - val_loss: 2.7163 - val_mae: 1.1676\n\nEpoch 00020: val_loss did not improve from 2.45682\nEpoch 21/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.9522 - mae: 0.9638 - val_loss: 2.4633 - val_mae: 1.1338\n\nEpoch 00021: val_loss did not improve from 2.45682\nEpoch 22/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.8735 - mae: 0.9707 - val_loss: 2.4779 - val_mae: 1.1206\n\nEpoch 00022: val_loss did not improve from 2.45682\nEpoch 23/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.9510 - mae: 0.9755 - val_loss: 2.3777 - val_mae: 1.1035\n\nEpoch 00023: val_loss improved from 2.45682 to 2.37767, saving model to models/model4.h5\nEpoch 24/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.7966 - mae: 0.9327 - val_loss: 2.4131 - val_mae: 1.1042\n\nEpoch 00024: val_loss did not improve from 2.37767\nEpoch 25/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6632 - mae: 0.9161 - val_loss: 2.4891 - val_mae: 1.1264\n\nEpoch 00025: val_loss did not improve from 2.37767\nEpoch 26/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5207 - mae: 0.8882 - val_loss: 2.4556 - val_mae: 1.1239\n\nEpoch 00026: val_loss did not improve from 2.37767\nEpoch 27/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6336 - mae: 0.8904 - val_loss: 2.4038 - val_mae: 1.1093\n\nEpoch 00027: val_loss did not improve from 2.37767\nEpoch 28/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6480 - mae: 0.9035 - val_loss: 2.5250 - val_mae: 1.1289\n\nEpoch 00028: val_loss did not improve from 2.37767\nEpoch 29/200\n322/322 [==============================] - 2s 7ms/step - loss: 1.8434 - mae: 0.9594 - val_loss: 2.5356 - val_mae: 1.1219\n\nEpoch 00029: val_loss did not improve from 2.37767\nEpoch 30/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6311 - mae: 0.9140 - val_loss: 2.4982 - val_mae: 1.1227\n\nEpoch 00030: val_loss did not improve from 2.37767\nEpoch 31/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5136 - mae: 0.8786 - val_loss: 2.8210 - val_mae: 1.1969\n\nEpoch 00031: val_loss did not improve from 2.37767\nEpoch 32/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.6333 - mae: 0.9065 - val_loss: 2.5015 - val_mae: 1.1274\n\nEpoch 00032: val_loss did not improve from 2.37767\nEpoch 33/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5947 - mae: 0.8994 - val_loss: 2.5975 - val_mae: 1.1604\n\nEpoch 00033: val_loss did not improve from 2.37767\nEpoch 34/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.5836 - mae: 0.8646 - val_loss: 2.4735 - val_mae: 1.1286\n\nEpoch 00034: val_loss did not improve from 2.37767\nEpoch 35/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3265 - mae: 0.8126 - val_loss: 2.4498 - val_mae: 1.1125\n\nEpoch 00035: val_loss did not improve from 2.37767\nEpoch 36/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3671 - mae: 0.8008 - val_loss: 2.4407 - val_mae: 1.1096\n\nEpoch 00036: val_loss did not improve from 2.37767\nEpoch 37/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.3028 - mae: 0.8036 - val_loss: 2.4563 - val_mae: 1.1186\n\nEpoch 00037: val_loss did not improve from 2.37767\nEpoch 38/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1697 - mae: 0.7666 - val_loss: 2.4315 - val_mae: 1.1097\n\nEpoch 00038: val_loss did not improve from 2.37767\nEpoch 39/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1505 - mae: 0.7567 - val_loss: 2.4717 - val_mae: 1.1168\n\nEpoch 00039: val_loss did not improve from 2.37767\nEpoch 40/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.2166 - mae: 0.7702 - val_loss: 2.4798 - val_mae: 1.1241\n\nEpoch 00040: val_loss did not improve from 2.37767\nEpoch 41/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1688 - mae: 0.7445 - val_loss: 2.4785 - val_mae: 1.1196\n\nEpoch 00041: val_loss did not improve from 2.37767\nEpoch 42/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.1323 - mae: 0.7423 - val_loss: 2.5867 - val_mae: 1.1448\n\nEpoch 00042: val_loss did not improve from 2.37767\nEpoch 43/200\n322/322 [==============================] - 2s 6ms/step - loss: 1.0914 - mae: 0.7489 - val_loss: 2.5239 - val_mae: 1.1275\n","output_type":"stream"},{"name":"stderr","text":"Folds: 100%|██████████| 5/5 [13:40<00:00, 164.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00043: val_loss did not improve from 2.37767\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"cv = 0.\nfor l in val_losses:\n    plt.plot(l)\n    cv += np.min(l)\n\nplt.title(f\"CV MSE loss: {cv / len(val_losses):.03f}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:48:38.42503Z","iopub.execute_input":"2022-12-30T11:48:38.426261Z","iopub.status.idle":"2022-12-30T11:48:39.107512Z","shell.execute_reply.started":"2022-12-30T11:48:38.426204Z","shell.execute_reply":"2022-12-30T11:48:39.106536Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'CV MSE loss: 2.710')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"import glob\nthermonet_models = [load_model(f) for f in glob.glob(f'{MODELS_PATH}/model*.h5')]","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:48:42.177635Z","iopub.execute_input":"2022-12-30T11:48:42.178779Z","iopub.status.idle":"2022-12-30T11:48:42.68808Z","shell.execute_reply.started":"2022-12-30T11:48:42.178736Z","shell.execute_reply":"2022-12-30T11:48:42.687057Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"TEST_FEATURES_PATH = '../input/thermonet-features/nesp_features.npy'\nif not os.path.exists(TEST_FEATURES_PATH):    \n    np.save(thermonet_features(df_test.query('op == \"replace\"')), 'test_features.npy')\n    TEST_FEATURES_PATH = 'test_features.npy'","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:48:45.860537Z","iopub.execute_input":"2022-12-30T11:48:45.860939Z","iopub.status.idle":"2022-12-30T11:48:45.87261Z","shell.execute_reply.started":"2022-12-30T11:48:45.860902Z","shell.execute_reply":"2022-12-30T11:48:45.87156Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_features = np.load(TEST_FEATURES_PATH)\ntest_features = np.moveaxis(test_features, 1, -1)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:48:48.492695Z","iopub.execute_input":"2022-12-30T11:48:48.493086Z","iopub.status.idle":"2022-12-30T11:48:55.56937Z","shell.execute_reply.started":"2022-12-30T11:48:48.493054Z","shell.execute_reply":"2022-12-30T11:48:55.568266Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_ddg = np.stack([model.predict(test_features) for model in thermonet_models])\ntest_ddg = np.mean(test_ddg, axis=0).flatten()\ntest_ddg","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:49:06.07653Z","iopub.execute_input":"2022-12-30T11:49:06.07761Z","iopub.status.idle":"2022-12-30T11:49:15.106669Z","shell.execute_reply.started":"2022-12-30T11:49:06.077563Z","shell.execute_reply":"2022-12-30T11:49:15.105662Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([0.5510038 , 0.56166923, 0.715446  , ..., 0.2220248 , 0.13028903,\n       0.19862112], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"df_test.loc[df_test.op == 'replace', 'ddg'] = -test_ddg\ndf_test.loc[df_test['op'] == \"delete\", 'ddg'] = df_test[df_test[\"op\"]==\"replace\"][\"ddg\"].quantile(q=0.25)\ndf_test.loc[df_test['op'] == \"same\", 'ddg'] = 0.\ndf_test.rename(columns={'ddg': 'tm'})[['seq_id', 'tm']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:57:05.252714Z","iopub.execute_input":"2022-12-30T11:57:05.253116Z","iopub.status.idle":"2022-12-30T11:57:05.278908Z","shell.execute_reply.started":"2022-12-30T11:57:05.253083Z","shell.execute_reply":"2022-12-30T11:57:05.277919Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:57:32.021694Z","iopub.execute_input":"2022-12-30T11:57:32.023042Z","iopub.status.idle":"2022-12-30T11:57:33.055796Z","shell.execute_reply.started":"2022-12-30T11:57:32.023Z","shell.execute_reply":"2022-12-30T11:57:33.054553Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"seq_id,tm\n31390,-0.5510038\n31391,-0.56166923\n31392,-1.2458285\n31393,-0.715446\n31394,-0.96589696\n31395,-1.2597075\n31396,-0.8344709\n31397,-0.6182085\n31398,-0.6765175\n","output_type":"stream"}]}]}